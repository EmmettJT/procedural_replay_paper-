{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616c9c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Make notebook wider:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e172d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def load_H5_bodypart(tracking_path,video_type, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'task' in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "    \n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "    \n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "  \n",
    "def load_H5_ports(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'port' in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "    \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "def clean_and_interpolate(head_centre,threshold):\n",
    "    bad_confidence_inds = np.where(head_centre.likelihood.values<threshold)[0]\n",
    "    newx = head_centre.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = head_centre.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    head_centre['interped_x'] = interped_x\n",
    "    head_centre['interped_y'] = interped_y\n",
    "    \n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                #print('interp_start_index: ', interp_start_index)\n",
    "                #print('interp_start_value: ', value_before)\n",
    "                #print('')\n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                    #print('interp_end_index: ', interp_end_index)\n",
    "                    #print('interp_end_value: ', value_after)\n",
    "                    #print('')\n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                    #print('interp_diff_index is:', interp_diff_index)\n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        #print('interp_index is:', interp_index)\n",
    "                        #print('new_value should be:', new_values[x])\n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "#     print('function exiting')\n",
    "    return(coords_list)\n",
    "\n",
    "def sortperm_neurons(bkgd_log_proportions_array,config,neuron_response_df, sequence_ordering=None, th=0.2):\n",
    "    ## this is number of neurons in total\n",
    "    N_neurons= bkgd_log_proportions_array.shape[1]\n",
    "    ## number of sequences from json file \n",
    "    n_sequences = config[\"num_sequence_types\"]\n",
    "    # the 18 neuron params for each neuron from the last iteration\n",
    "    all_final_globals = neuron_response_df.iloc[-N_neurons:]\n",
    "    # this cuts it down to just the first 6 params - i think this correspond sto the first param for each seq type? response probABILITY - ie the chance that a neuron spikes in a given latent seq \n",
    "    resp_prop = np.exp(all_final_globals.values[:, :n_sequences])#\n",
    "    # this takes the next 6 params - which i think are the offset values\n",
    "    offset = all_final_globals.values[-N_neurons:, n_sequences:2*n_sequences]\n",
    "    ## finds the max response value - ie. which seq it fits to? \n",
    "    peak_response = np.amax(resp_prop, axis=1)\n",
    "    # then threshold the reponse\n",
    "    has_response = peak_response > np.quantile(peak_response, th)\n",
    "    # I thin this is the sequence that the neuron has the max response for: ie. we are ordering them by max response \n",
    "    preferred_type = np.argmax(resp_prop, axis=1)\n",
    "    if sequence_ordering is None:\n",
    "        # order them by max reponse \n",
    "        ordered_preferred_type = preferred_type\n",
    "    else:\n",
    "        #order them differnetly \n",
    "        ordered_preferred_type = np.zeros(N_neurons)#\n",
    "        # loop through each sequence\n",
    "        for seq in range(n_sequences):\n",
    "            # where does  max repsone = user defined seque\n",
    "            seq_indices = np.where(preferred_type == sequence_ordering[seq])\n",
    "            # change order to different seq\n",
    "            ordered_preferred_type[seq_indices] = seq\n",
    "\n",
    "    # reorder the offset params according to max respsone\n",
    "    preferred_delay = offset[np.arange(N_neurons), preferred_type]\n",
    "    Z = np.stack([has_response, ordered_preferred_type+1, preferred_delay], axis=1)\n",
    "    indexes = np.lexsort((Z[:, 2], Z[:, 1], Z[:, 0]))\n",
    "    return indexes,ordered_preferred_type\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def split_list(nums):\n",
    "    sublists = []\n",
    "    current_sublist = [nums[0]]\n",
    "    current_element = nums[0]\n",
    "    for i in range(1,len(nums)):\n",
    "        if nums[i] == current_element:\n",
    "            current_sublist.append(nums[i])\n",
    "        else:\n",
    "            sublists.append(current_sublist)\n",
    "            current_sublist = [nums[i]]\n",
    "            current_element = nums[i]\n",
    "    sublists.append(current_sublist)\n",
    "    return sublists\n",
    "\n",
    "def conactinate_nth_items(startlist):\n",
    "    concatinated_column_vectors = []\n",
    "    for c in range(len(max(startlist, key=len))):\n",
    "        column = []\n",
    "        for t in range(len(startlist)):\n",
    "            if c <= len(startlist[t])-1:\n",
    "                column = column + [startlist[t][c]]\n",
    "        concatinated_column_vectors.append(column)\n",
    "    return concatinated_column_vectors\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def combine_list_strings(list_):\n",
    "    string = ''\n",
    "    for item in list_:\n",
    "        string += item + '_'\n",
    "    return string[0:-1]\n",
    "\n",
    "def insert_row(idx, df, df_insert):\n",
    "    return df.iloc[:idx, ].append(df_insert).append(df.iloc[idx:, ]).reset_index(drop = True)\n",
    "\n",
    "def fix_missing_neurons(spikes_df):\n",
    "\n",
    "    count = 0\n",
    "    neuron_ids = spikes_df.neuron.values\n",
    "\n",
    "    for index,_id_ in enumerate(neuron_ids[:-1]):\n",
    "        if index == 0:\n",
    "            if not _id_ == 1:\n",
    "                new_row = pd.DataFrame({'neuron': [1.0], 'timestamp':[np.nan], 'sequence_type':[np.nan],  'seq_confidence': [np.nan], 'sequence_type_adjusted':[np.nan] })\n",
    "                spikes_df= new_row.append(spikes_df)\n",
    "                print(index)\n",
    "\n",
    "                count +=1\n",
    "\n",
    "        if not _id_ == neuron_ids[index+1]:\n",
    "            if not _id_ + 1 == neuron_ids[index+1]:\n",
    "                new_row = pd.DataFrame({'neuron': [_id_+1], 'timestamp':[np.nan], 'sequence_type':[np.nan],  'seq_confidence': [np.nan], 'sequence_type_adjusted':[np.nan] })\n",
    "                insert_row(index,spikes_df,new_row)\n",
    "                count +=1\n",
    "\n",
    "    print(str(count) + ' neurons were missing - did not fire a spike' )\n",
    "\n",
    "    return spikes_df.reset_index()\n",
    "\n",
    "\n",
    "def return_inds_for_seq_groups(lst):\n",
    "    groups = []\n",
    "    peak_inds = []\n",
    "    new = True\n",
    "    for ind,item in enumerate(lst):\n",
    "        if new:\n",
    "            if item > 0:\n",
    "                start = ind\n",
    "                new = False\n",
    "                peak_ind = ind\n",
    "        else:\n",
    "            if item > lst[peak_ind]:\n",
    "                peak_ind = ind\n",
    "            if item == 0:\n",
    "                end = ind\n",
    "                if not start == 0:\n",
    "                    groups.append((start-1, end))\n",
    "                else:\n",
    "                    groups.append((start, end))    \n",
    "                peak_inds.append(peak_ind)\n",
    "                new = True\n",
    "    return groups,peak_inds\n",
    "\n",
    "def find_missing_elements(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    missing_elements = set1 - set2\n",
    "    return list(missing_elements)\n",
    "\n",
    "# ### if two found rectangles overlap with a single motif then remove the smallest one\n",
    "# def remove_overlapping_rectangles(overlap_proportion, implanted_index):\n",
    "#     repeat_index = []\n",
    "    \n",
    "#     for i, index in enumerate(implanted_index):\n",
    "#         if i > 0 and index == implanted_index[i - 1]:\n",
    "#             repeat_index.append(i)\n",
    "    \n",
    "#     new_overlap_proportion = overlap_proportion.copy()\n",
    "#     new_implanted_index = implanted_index.copy()\n",
    "    \n",
    "#     for index in repeat_index[::-1]:\n",
    "#         a = overlap_proportion[index]\n",
    "#         b = overlap_proportion[index - 1]\n",
    "        \n",
    "#         if a > b:\n",
    "#             new_overlap_proportion.pop(index - 1)\n",
    "#             new_implanted_index.pop(index - 1)\n",
    "#         else:\n",
    "#             new_overlap_proportion.pop(index)\n",
    "#             new_implanted_index.pop(index)\n",
    "    \n",
    "#     return new_overlap_proportion, new_implanted_index\n",
    "\n",
    "\n",
    "### if two found rectangles overlap with a single motif then remove the smallest one\n",
    "def remove_overlapping_rectangles(overlap_proportion, implanted_index,actually_identified_seq_types):\n",
    "    repeat_index = []\n",
    "    \n",
    "    for i, index in enumerate(implanted_index):\n",
    "        if i > 0 and index == implanted_index[i - 1]:\n",
    "            repeat_index.append(i)\n",
    "    \n",
    "    new_overlap_proportion = overlap_proportion.copy()\n",
    "    new_implanted_index = implanted_index.copy()\n",
    "    new_actually_identified_seq_types = actually_identified_seq_types.copy()\n",
    "    \n",
    "    for index in repeat_index[::-1]:\n",
    "        a = overlap_proportion[index]\n",
    "        b = overlap_proportion[index - 1]\n",
    "        \n",
    "        if a > b:\n",
    "            new_overlap_proportion.pop(index - 1)\n",
    "            new_implanted_index.pop(index - 1)\n",
    "            new_actually_identified_seq_types.pop(index - 1)\n",
    "        else:\n",
    "            new_overlap_proportion.pop(index)\n",
    "            new_implanted_index.pop(index)\n",
    "            new_actually_identified_seq_types.pop(index)\n",
    "    \n",
    "    return new_overlap_proportion, new_implanted_index,new_actually_identified_seq_types\n",
    "\n",
    "def area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "    dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fdfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on 1 files that were missing\n",
      "1/2 ---------------------------------------------------------\n",
      "standard_insertion\n",
      "standard_insertion\n",
      "loading\n",
      "created folder :  Z:\\projects\\Emmett\\synthetic_data_rerun\\ppseq_output\\\\136_1_3_standard_insertion_run_2011023_1215/analysis_output\n",
      "Z:\\projects\\Emmett\\synthetic_data_rerun\\ppseq_output\\\\136_1_3_standard_insertion_run_2011023_1215\n",
      "LOADING PPSEQ DATA\n",
      "\n",
      "\n",
      "      done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.17it/s]\n",
      " 27%|███████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                    | 47053/176386 [00:14<00:38, 3402.56it/s]"
     ]
    }
   ],
   "source": [
    "m_i_rs = ['136_1_3']\n",
    "          \n",
    "#         '148_2_2','149_1_1','178_1_6','178_1_7','178_2_1','178_2_3','178_2_4','269_1_2','270_1_3']\n",
    "\n",
    "## PATHS\n",
    "for mouse_session_recording in m_i_rs:\n",
    "    \n",
    "    ## check if files ahve already been run and if so skip these:\n",
    "    synth_save_path = r\"Z:\\projects\\Emmett\\synthetic_data_rerun\\synthetic_spikes\\\\\"\n",
    "    ppseq_out_path = \"Z:\\projects\\Emmett\\synthetic_data_rerun\\ppseq_output\\\\\"\n",
    "    data_name = []\n",
    "    for file_dir in os.listdir(ppseq_out_path):\n",
    "        if mouse_session_recording in file_dir:\n",
    "            string = ''\n",
    "            for item in file_dir.split('_')[3:-3]:\n",
    "                string += item + '_'\n",
    "            data_name += [string[0:-1]]\n",
    "\n",
    "    output_data_file_path = synth_save_path + \"\\\\\" + mouse_session_recording + '\\\\' + mouse_session_recording + \"_proportions_found.csv\"\n",
    "\n",
    "    if os.path.isfile(output_data_file_path):\n",
    "        output_df = pd.read_csv(output_data_file_path,index_col=0)\n",
    "        output_df = output_df.reset_index(drop =True)\n",
    "        new_data_name = []\n",
    "        for d_n in data_name:\n",
    "            if not d_n in output_df.type.values:\n",
    "                new_data_name += [d_n]\n",
    "\n",
    "        if len(new_data_name) == 0:\n",
    "            print('none to run')\n",
    "        else:\n",
    "            print('running on ' + str(len(new_data_name)) + ' files that were missing')\n",
    "        data_name = new_data_name\n",
    "\n",
    "    else:\n",
    "        print('creating new file')\n",
    "        # create output df:\n",
    "        output_df = pd.DataFrame(\n",
    "            {'mouse_implant_recording' : [],\n",
    "            'type' : [],\n",
    "            'proportion_found' : [],\n",
    "            'proportion_mislabeled': [],\n",
    "            'seqs_implanted':[],\n",
    "            'seqs_found_over_implanted':[]})\n",
    "        output_df.to_csv(output_data_file_path)\n",
    "\n",
    "    for run_index,session_name in enumerate(data_name):\n",
    "\n",
    "        if not run_index == 19:\n",
    "            print(str(run_index+1) + '/' + str(len(data_name)) +' ---------------------------------------------------------')\n",
    "            print(session_name)\n",
    "\n",
    "            PP_PATH = r\"Z:\\projects\\Emmett\\synthetic_data_rerun\\ppseq_output\\\\\"\n",
    "\n",
    "            implanted_data_path= r\"Z:\\projects\\Emmett\\synthetic_data_rerun\\synthetic_spikes\\\\\" + mouse_session_recording + \"\\\\\"\n",
    "            for item in os.listdir(implanted_data_path):\n",
    "                current_file = combine_list_strings(item.split('_')[0:len(session_name.split('_'))])\n",
    "                if session_name == current_file:\n",
    "                    print(item)\n",
    "                    implanted_data_path = implanted_data_path + item + '\\\\ppseq_events.pkl'\n",
    "\n",
    "\n",
    "            ##  load in ground truth implanted data: \n",
    "            # standard insertion: \n",
    "\n",
    "            if not session_name == 'blank_test':\n",
    "                print('loading')\n",
    "                implanted_data = pd.read_pickle(implanted_data_path)\n",
    "\n",
    "            DAT_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "\n",
    "            ## set ppseq file\n",
    "            for file_ in os.listdir(PP_PATH):\n",
    "                if session_name in file_:\n",
    "                    if mouse_session_recording in file_:\n",
    "                        file = file_\n",
    "\n",
    "            ## set dat_path:\n",
    "            for file_ in os.listdir(DAT_PATH):\n",
    "                if mouse_session_recording.split('_')[0] in file_:\n",
    "                    if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                        dat_path = os.path.join(DAT_PATH,file_)\n",
    "            for recording in os.listdir(os.path.join(DAT_PATH,dat_path)):\n",
    "                if recording.split('_')[0][-1] == mouse_session_recording.split('_')[-1]:\n",
    "                    dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "            # set tracking path\n",
    "            for file_ in os.listdir(dat_path + r\"\\video\\tracking\\\\\"):\n",
    "                if 'task' in file_:\n",
    "                    tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\",file_) + '\\\\'\n",
    "            # set video paths\n",
    "            for file_ in os.listdir(dat_path + r\"\\video\\videos\\\\\"):\n",
    "                if 'task' in file_:\n",
    "                    cam_path = os.path.join(dat_path + r\"\\video\\videos\\\\\",file_) + '\\\\' \n",
    "                    for vid_file in os.listdir(cam_path):\n",
    "                        if 'back' in vid_file and '.avi' in vid_file:\n",
    "                            back_cam_avi_path = os.path.join(cam_path,vid_file)\n",
    "                        if 'side' in vid_file and '.avi' in vid_file:\n",
    "                            side_cam_avi_path = os.path.join(cam_path,vid_file)\n",
    "\n",
    "            # set save paths:\n",
    "            PP_save_path = PP_PATH + file + '/analysis_output'\n",
    "\n",
    "\n",
    "            CHECK_FOLDER = os.path.isdir(PP_save_path)\n",
    "            # If folder doesn't exist, then create it.\n",
    "            if not CHECK_FOLDER:\n",
    "                os.makedirs(PP_save_path)\n",
    "                print(\"created folder : \", PP_save_path)\n",
    "\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------------------------------------------                    \n",
    "\n",
    "            ## LOAD \n",
    "            print(PP_PATH + file)\n",
    "            print(\"LOADING PPSEQ DATA\")\n",
    "            print('\\n')\n",
    "            #The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\n",
    "            assignment_history_df = pd.read_csv(PP_PATH + file + r\"\\assigment_hist_frame.csv\")\n",
    "\n",
    "            # latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\n",
    "            latent_event_history_df = pd.read_csv(PP_PATH + file + r\"\\latent_event_hist.csv\")\n",
    "\n",
    "            # seq_type_log_proportions: log p of each type of sequence at each iteration\n",
    "            seq_type_log_proportions_df = pd.read_csv(PP_PATH + file + r\"\\seq_type_log_proportions.csv\")\n",
    "\n",
    "            # neuron_responses.csv: iterations x neurons by 3(number of sequences). Each neuron has three parameters per sequence to describe how it is influenced by each sequence type. \n",
    "            # Each iteration these are resampled, therefore there are number of neurons by iterations by 3 by number of sequences of these numbers.\n",
    "            neuron_response_df = pd.read_csv(PP_PATH + file + r\"\\neuron_response.csv\")\n",
    "\n",
    "            #log_p_hist.csv: the history of the log_p of the model\n",
    "            log_p_hist_df = pd.read_csv(PP_PATH + file + r\"\\log_p_hist.csv\")\n",
    "\n",
    "            # unmasked_spikes_df = pd.read_csv(PP_PATH + file + r\"\\unmasked_spikes.csv\")\n",
    "\n",
    "            ## load in spikes \n",
    "            spikes_file = os.path.join(PP_PATH + file,'trainingData\\\\') + '_'.join(file.split('_')[0:-3]) + '.txt'\n",
    "\n",
    "            neuron_ids, spike_times= [], []\n",
    "            with open(spikes_file) as f:\n",
    "                for (i, line) in enumerate(f.readlines()):\n",
    "                    [neuron_id, spike_time] = line.split(' ', 1)\n",
    "                    spike_time = eval(spike_time.split('\\n')[0])\n",
    "                    neuron_id = eval(neuron_id.split('\\t')[0])\n",
    "                    spike_times.append(spike_time)\n",
    "                    neuron_ids.append(neuron_id)\n",
    "\n",
    "            spikes_df = pd.DataFrame({'neuron':neuron_ids,'timestamp':spike_times}) \n",
    "\n",
    "\n",
    "            bkgd_log_proportions_array = pd.read_csv(PP_PATH + file + r\"\\bkgd_log_proportions_array.csv\")\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open(PP_PATH + file + r'\\config_file.json')\n",
    "\n",
    "            # returns JSON object as \n",
    "            # a dictionary\n",
    "            config = eval(json.load(f))\n",
    "\n",
    "            print(f'      done')\n",
    "\n",
    "            ## load in the timespan used for pppseq:\n",
    "            string = ''\n",
    "            for item in file.split('_')[0:-3]:\n",
    "                string += item + '_'\n",
    "            STRING = string[0:-1]\n",
    "            input_params_path = os.path.join(PP_PATH + file,'trainingData\\\\') + ('params_' + STRING +'.json')\n",
    "            # Opening JSON file\n",
    "            f = open(input_params_path)\n",
    "            # returns JSON object as a dictionary\n",
    "            input_config = json.load(f)\n",
    "            behav_time_interval_start = input_config['time_span'][0]\n",
    "\n",
    "\n",
    "\n",
    "            time_spans = [[0,600]]\n",
    "            behav_time_interval_start = time_spans[0]\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "            plt.close()\n",
    "            #find max value\n",
    "\n",
    "            # find 95% of growth value and when it crossed this\n",
    "            max_ = max(log_p_hist_df.x1)\n",
    "            min_ = min(log_p_hist_df.x1)\n",
    "            growth = max_ - min_\n",
    "            _prcntile =  max_ - (0.02 * growth)\n",
    "\n",
    "            ## model log likley hood curve\n",
    "            plt.plot(log_p_hist_df.x1)\n",
    "            plt.axhline(y=_prcntile, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "            SaveFig('/log_l_curve.png',PP_save_path)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            ## macth seq id to seq type across itterations\n",
    "\n",
    "            # Initialize an empty df to store the result\n",
    "            seq_types_df = pd.DataFrame()\n",
    "\n",
    "            # Iterate through the range\n",
    "            # for iteration_ in tqdm(range(400, 500)):\n",
    "            for iteration_ in tqdm(range(200,300)):\n",
    "\n",
    "                # Extract the relevant column from the assignment history dataframe\n",
    "                assignment_history_df_split = assignment_history_df[str(list(assignment_history_df)[iteration_])]\n",
    "\n",
    "                # Get the index of the -1 split markers in the latent event history dataframe\n",
    "                end_markers = latent_event_history_df.loc[latent_event_history_df['seq_type'] == -1.0].index\n",
    "                # Extract the relevant portion of the latent event history dataframe\n",
    "                latent_event_history_df_split =  latent_event_history_df[end_markers[iteration_-1]:end_markers[iteration_]]\n",
    "\n",
    "                # Create a dictionary from the dataframe for faster lookups\n",
    "                df_dict = latent_event_history_df_split.set_index('assignment_id')['seq_type'].to_dict()\n",
    "\n",
    "                # Match the sequence ID to the sequence type\n",
    "                seq_type = find_corresponding(assignment_history_df_split)\n",
    "\n",
    "                # Append the result to the df\n",
    "                seq_types_df[str(iteration_+1)] = seq_type\n",
    "\n",
    "\n",
    "            proportion = []\n",
    "            seq_type = []\n",
    "            for index in tqdm(range(len(seq_types_df))):\n",
    "                row = seq_types_df.loc[index]\n",
    "                seq_type += [statistics.mode(row)] \n",
    "                proportion += [np.count_nonzero(row == statistics.mode(row)) / len(row)]\n",
    "\n",
    "            # add seq type to dataframe\n",
    "            spikes_df['sequence_type'] = seq_type\n",
    "            # add seq type to dataframe\n",
    "            spikes_df['seq_confidence'] = proportion\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            ## filter for background confidence :\n",
    "\n",
    "            # thresh = max(proportion) *.75\n",
    "            thresh = max(proportion) *0.75\n",
    "            plt.plot(np.sort(proportion)[::-1])\n",
    "            plt.axhline(y = thresh, color = 'r', linestyle = '-')\n",
    "\n",
    "            SaveFig('/spike_conf_filter.png',PP_save_path)\n",
    "\n",
    "            spikes_df['sequence_type_adjusted'] = seq_type\n",
    "            spikes_df.sequence_type_adjusted[np.where(spikes_df.seq_confidence < thresh)[0]] = -1\n",
    "\n",
    "            # its possible that some neurons have no spikes for the given time frame so are not in the list, add htem in to avoid sorting issues and give their spike times as nan \n",
    "            spikes_df = fix_missing_neurons(spikes_df)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            # for loading color and order data from awake alignment \n",
    "            finalised_data_path = r'Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\'\n",
    "            for item in os.listdir(finalised_data_path):\n",
    "                if mouse_session_recording in item:\n",
    "                    finalised_data_path= finalised_data_path + item \n",
    "\n",
    "            finalised_data_path = finalised_data_path + '\\\\analysis_output/reordered_recolored//'\n",
    "\n",
    "            import pickle as pickle\n",
    "\n",
    "            with open(finalised_data_path + \"colors\", \"rb\") as input_file:\n",
    "                colors = pickle.load(input_file)\n",
    "\n",
    "            with open(finalised_data_path + \"ordering\", \"rb\") as input_file:\n",
    "                ordering = pickle.load(input_file)\n",
    "\n",
    "            with open(finalised_data_path + \"neuron_index\", \"rb\") as input_file:\n",
    "                neuron_index = pickle.load(input_file)\n",
    "\n",
    "            with open(finalised_data_path + \"ordered_preferred_type\", \"rb\") as input_file:\n",
    "                ordered_preferred_type = pickle.load(input_file)\n",
    "\n",
    "\n",
    "            # # for 8 seqs:\n",
    "            colors += ['pink','lightblue', 'k'] \n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------        \n",
    "\n",
    "            %matplotlib inline\n",
    "\n",
    "\n",
    "            ### Plot sequences - basic\n",
    "            timeframe = [0,600]\n",
    "            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "\n",
    "            ## neuron order:\n",
    "\n",
    "            #define neuron order\n",
    "            neuron_permute_loc = np.zeros(len(neuron_index))\n",
    "            for i in range(len(neuron_index)):\n",
    "                neuron_permute_loc[i] = int(list(neuron_index).index(i))\n",
    "            neuron_order = neuron_permute_loc[(spikes_df.neuron-1).astype(int)]\n",
    "\n",
    "\n",
    "            ## plotting:\n",
    "\n",
    "            fig, [ax,ax2] = plt.subplots(2, 1,figsize=(40, 10))\n",
    "\n",
    "            # plot background in grey \n",
    "            background_keep_mask = []\n",
    "            for item in spikes_df[mask].sequence_type_adjusted:\n",
    "                if item < 0 or item >= 7.0:\n",
    "                    background_keep_mask.append(True)\n",
    "                else:\n",
    "                    background_keep_mask.append(False)\n",
    "            background_keep_mask = np.array(background_keep_mask)\n",
    "            ax.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],marker = 'o', s=40, linewidth=0,color = 'lightgrey' ,alpha=0.3)\n",
    "            c_ = np.array(colors)[spikes_df[mask][background_keep_mask].sequence_type_adjusted.values.astype(int)]\n",
    "            ax2.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=0.3)\n",
    "            ax2.set_title('extra sequences and bakcground only')\n",
    "            # plot spikes without background\n",
    "            background_remove_mask = spikes_df[mask].sequence_type_adjusted >= 0\n",
    "            background_remove_mask = (spikes_df[mask].sequence_type_adjusted >= 0)*(spikes_df[mask].sequence_type_adjusted != 7.0)*(spikes_df[mask].sequence_type_adjusted != 8.0)\n",
    "            c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "            # ## faster:\n",
    "            ax.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "            ax.set_title('held sequences in colour and extra sequences + background in grey')\n",
    "\n",
    "            SaveFig('/replay_background_and_extraseqs.png',PP_save_path)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            timeframe = [0,60]\n",
    "\n",
    "            #mask\n",
    "            # spikemask\n",
    "            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "\n",
    "            fig, [ax1,ax2,ax3] = plt.subplots(3, 1,figsize=(30, 15))\n",
    "\n",
    "            1 ### plot ordered ppseq spikes as above:\n",
    "\n",
    "            # plot background in grey \n",
    "            background_keep_mask = spikes_df[mask].sequence_type_adjusted <= 0\n",
    "            ax1.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],marker = 'o', s=40, linewidth=0,color = 'grey' ,alpha=0.5)\n",
    "            # mask to remove background neurons  and extra seq neurons from the color plot\n",
    "            background_remove_mask = (spikes_df[mask].sequence_type_adjusted >= 0) * (spikes_df[mask].sequence_type_adjusted <= 6.0)\n",
    "            c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "            ax1.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "\n",
    "            ### bin the spiking and plot number of spikes for each seq type\n",
    "\n",
    "            ignore_background_seqs = (spikes_df.sequence_type_adjusted >= 0) * (spikes_df.sequence_type_adjusted <= 6.0)\n",
    "            seqs = np.unique(spikes_df.sequence_type_adjusted[ignore_background_seqs])\n",
    "            seq_spikes = []\n",
    "            for seq_type_ in seqs:  \n",
    "                seq_spikes += [spikes_df.timestamp[np.where(spikes_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "\n",
    "            # Define the bin size (in this case, 0.2s)\n",
    "            bin_size = 0.2\n",
    "\n",
    "            seq_spike_occurance = []\n",
    "            for spikes_ in seq_spikes:\n",
    "                # Use the numpy.histogram function to bin the data\n",
    "                hist, bins = np.histogram(spikes_, bins=np.arange(0, np.diff(behav_time_interval_start)[0], bin_size))\n",
    "                seq_spike_occurance += [list(hist)]\n",
    "\n",
    "            strt_ = int(timeframe[0]/bin_size)\n",
    "            end_ = int(timeframe[1]/bin_size)\n",
    "\n",
    "            # seqs = [-1] + list(seqs)\n",
    "            # seq_spike_occurance = [(len(seq_spike_occurance[0]) * [0])] + seq_spike_occurance\n",
    "            for i in range (len(seq_spike_occurance)):\n",
    "                print(seqs[i])\n",
    "                ax2.plot(seq_spike_occurance[i][strt_:end_], c = colors[int(seqs[i])])\n",
    "\n",
    "            skip = False\n",
    "            if len(seq_spike_occurance) == 0:\n",
    "                skip = True\n",
    "            if session_name == 'blank_test':\n",
    "                skip = True\n",
    "\n",
    "            if not skip == True:\n",
    "\n",
    "                ## plot the current seq type (based on binned spiking):\n",
    "                nth_vectors = conactinate_nth_items(seq_spike_occurance)\n",
    "                max_index= []\n",
    "                for sublist in nth_vectors: \n",
    "                    if sum(sublist) == 0:\n",
    "                        max_index += [-1]\n",
    "                    else:\n",
    "                        inde_ = seqs[np.argmax(sublist)]\n",
    "                        max_index += [int(inde_)]\n",
    "\n",
    "                # ax2.plot(max_index[strt_:end_])\n",
    "\n",
    "                ax2.scatter(np.linspace(0,end_ - strt_-1,end_ - strt_),np.ones(end_ - strt_)*-10, marker = '_',color = np.array(colors)[(np.array(max_index))[strt_:end_]], s = 100)\n",
    "\n",
    "                ### plot the ppseq latent:\n",
    "\n",
    "                # mask for the timespan\n",
    "                latent_timespan_mask = (latent_event_history_df_split.timestamp>timeframe[0])*(latent_event_history_df_split.timestamp<timeframe[-1])\n",
    "\n",
    "                # ignore anything that is below threshold * max amplitude\n",
    "                thr_ = 0\n",
    "                thresh_value = max(latent_event_history_df_split.amplitude) * thr_\n",
    "\n",
    "\n",
    "                amplitude_mask = latent_event_history_df_split[latent_timespan_mask].amplitude > thresh_value\n",
    "\n",
    "                c_l = np.array(colors)[latent_event_history_df_split[latent_timespan_mask][amplitude_mask].seq_type.values.astype(int)]\n",
    "                ax3.scatter(latent_event_history_df_split[latent_timespan_mask][amplitude_mask].timestamp,latent_event_history_df_split[latent_timespan_mask][amplitude_mask].seq_type ,marker = 'o', s=40, linewidth=0,color = c_l ,alpha=1)\n",
    "\n",
    "                #     SaveFig('/seq_extraction.png',PP_save_path)\n",
    "\n",
    "\n",
    "\n",
    "                # determine when sequences happen - filter etc. \n",
    "                # plot numbers found. \n",
    "                # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "\n",
    "                from matplotlib.patches import Rectangle\n",
    "                from collections import namedtuple\n",
    "                Rectangle_ = namedtuple('Rectangle_', 'xmin ymin xmax ymax')\n",
    "\n",
    "\n",
    "\n",
    "                min_spikes_threshold = 5\n",
    "\n",
    "\n",
    "                left = []\n",
    "                right = []\n",
    "                top = []\n",
    "                bottom = []\n",
    "                identified_seq_type = []\n",
    "\n",
    "                col_2 = []\n",
    "                rects2 = []\n",
    "\n",
    "                identified_seq_type = []\n",
    "                identified_rectangles = []\n",
    "\n",
    "                for i in range(len(seq_spike_occurance)):\n",
    "                    current_seq = seqs[i]\n",
    "                    seq_spike_count = seq_spike_occurance[i]\n",
    "                    # find seq start and end, defined by whetehr there were spikes or not \n",
    "                    groups, peaks = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "                    spikes_per_group = []\n",
    "                    for group in groups:\n",
    "                        timeframe = [(group[0]*bin_size),(group[-1]*bin_size)]\n",
    "                        mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "                        spikes_present = list(spikes_df[mask].sequence_type_adjusted.values.astype(int))\n",
    "                        spikes_per_group += [spikes_present.count(current_seq)]\n",
    "\n",
    "                        if spikes_present.count(current_seq) > min_spikes_threshold:\n",
    "                            seq_spikes_mask = spikes_df[mask].sequence_type == current_seq\n",
    "                            left += [min(spikes_df[mask][seq_spikes_mask].timestamp)]\n",
    "                            right += [max(spikes_df[mask][seq_spikes_mask].timestamp)]\n",
    "                            current_neuron_order = neuron_permute_loc[(spikes_df[mask][seq_spikes_mask].neuron.values-1).astype(int)]\n",
    "                            top += [max(current_neuron_order)]\n",
    "                            bottom += [min(current_neuron_order)]\n",
    "\n",
    "                            seq_type += [current_seq]\n",
    "\n",
    "                            l = min(spikes_df[mask][seq_spikes_mask].timestamp)\n",
    "                            r = max(spikes_df[mask][seq_spikes_mask].timestamp)\n",
    "                            current_neuron_order = neuron_permute_loc[(spikes_df[mask][seq_spikes_mask].neuron.values-1).astype(int)]\n",
    "                            t = max(current_neuron_order)\n",
    "                            b = min(current_neuron_order)\n",
    "\n",
    "                            col_ = colors[int(current_seq)]\n",
    "                            rects2 +=  [Rectangle((l, b), r-l, t-b, linewidth=1, edgecolor=col_, facecolor='none') ]\n",
    "\n",
    "                            identified_seq_type += [current_seq]\n",
    "                            identified_rectangles += [Rectangle_(l,b,r,t)]\n",
    "\n",
    "                filtered_groups_df = pd.DataFrame(\n",
    "                {'seq_type':identified_seq_type,\n",
    "                'left' : left,\n",
    "                'right' : right,\n",
    "                'top' :top,\n",
    "                 'bottom':bottom})\n",
    "\n",
    "\n",
    "                # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "                ##### plot this stuff to see if it makes sense\n",
    "\n",
    "                fig,[ax1,ax2] = plt.subplots(2, 1,figsize=(20, 10))\n",
    "\n",
    "                1 ### plot ordered ppseq spikes as above:\n",
    "\n",
    "                # plot background in grey \n",
    "                background_keep_mask = spikes_df.sequence_type_adjusted <= 0\n",
    "                ax1.scatter(spikes_df[background_keep_mask].timestamp, neuron_order[background_keep_mask],marker = 'o', s=40, linewidth=0,color = 'grey' ,alpha=0.5)\n",
    "                # mask to remove background neurons from the color plot\n",
    "                background_remove_mask = (spikes_df.sequence_type_adjusted >= 0) * (spikes_df.sequence_type_adjusted <= 6.0)\n",
    "                c_ = np.array(colors)[spikes_df[background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "                ax1.scatter(spikes_df[background_remove_mask].timestamp, neuron_order[background_remove_mask],marker = 'o', s=40, linewidth=0,color = 'grey' ,alpha=1)\n",
    "\n",
    "\n",
    "                from matplotlib.patches import Rectangle\n",
    "                col_ = []\n",
    "                rects = []\n",
    "                implanted_seq_types = []\n",
    "                all_implanted_rectangles = []\n",
    "                for index, seq_type in enumerate(implanted_data.seq_type.values):\n",
    "                    timestamps = implanted_data.inserted_profile[index].timestamp.values\n",
    "                    neurons = implanted_data.inserted_profile[index].neuron.values\n",
    "\n",
    "                    left = min(timestamps)\n",
    "                    right = max(timestamps)\n",
    "\n",
    "                    implanted_neuron_order = neuron_permute_loc[(neurons-1).astype(int)]\n",
    "                    top = max(implanted_neuron_order)\n",
    "                    bottom = min(implanted_neuron_order)\n",
    "\n",
    "                    col_ = colors[int(seq_type)]\n",
    "                    rects +=  [Rectangle((left, bottom), right-left, top-bottom, linewidth=1, edgecolor=col_, facecolor='none') ]\n",
    "\n",
    "                    implanted_seq_types += [seq_type]\n",
    "                    all_implanted_rectangles += [Rectangle_(left,bottom,right,top)]\n",
    "\n",
    "                ax1.set_title('implanted_seqs')\n",
    "                for rect in rects:\n",
    "                    ax1.add_patch(rect)\n",
    "\n",
    "                # now plot current seqs:\n",
    "                ax2.set_title('ppseq_identified_seqs')\n",
    "                background_keep_mask = spikes_df.sequence_type_adjusted <= 0\n",
    "                ax2.scatter(spikes_df[background_keep_mask].timestamp, neuron_order[background_keep_mask],marker = 'o', s=40, linewidth=0,color = 'grey' ,alpha=0.5)\n",
    "                # mask to remove background neurons from the color plot\n",
    "                background_remove_mask = (spikes_df.sequence_type_adjusted >= 0) * (spikes_df.sequence_type_adjusted <= 6.0)\n",
    "                c_ = np.array(colors)[spikes_df[background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "                ax2.scatter(spikes_df[background_remove_mask].timestamp, neuron_order[background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "                for rect_2 in rects2:\n",
    "                    ax2.add_patch(rect_2)\n",
    "\n",
    "                ax1.set_xlim(30,60)\n",
    "                ax2.set_xlim(30,60)\n",
    "\n",
    "                SaveFig('/identified-vs-implanted_seq_comparison.png',PP_save_path)\n",
    "\n",
    "                # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "                overlap_min_threshold = 0.1\n",
    "\n",
    "                overlap_proportion = []\n",
    "                implanted_index = []\n",
    "                mislablled_overlap_proportion = []\n",
    "                mislablled_implanted_index = []\n",
    "                actually_identified_seq_types = []\n",
    "                mislablled_identified_seq_types = []\n",
    "                for index,implanted_rect in enumerate(all_implanted_rectangles):\n",
    "\n",
    "                    #loop across identifed to see if any overlap:\n",
    "                    for i in range(len(identified_rectangles)):\n",
    "                        #if the seq type matches\n",
    "                        if identified_seq_type[i] == implanted_seq_types[index]:\n",
    "                            #calculate overlap area\n",
    "                            overlap_area = area(identified_rectangles[i],implanted_rect)\n",
    "                            if not overlap_area == None:\n",
    "                                length = implanted_rect.xmax - implanted_rect.xmin\n",
    "                                height = implanted_rect.ymax - implanted_rect.ymin\n",
    "                                area_implanted = length*height\n",
    "                                overlap_proportion += [overlap_area/area_implanted]\n",
    "                                implanted_index += [index]\n",
    "                                actually_identified_seq_types += [identified_seq_type[i]]\n",
    "                        else:\n",
    "                            #calculate overlap area\n",
    "                            overlap_area = area(identified_rectangles[i],implanted_rect)\n",
    "                            if not overlap_area == None:\n",
    "                                length = implanted_rect.xmax - implanted_rect.xmin\n",
    "                                height = implanted_rect.ymax - implanted_rect.ymin\n",
    "                                area_implanted = length*height\n",
    "                                mislablled_overlap_proportion += [overlap_area/area_implanted]\n",
    "                                mislablled_implanted_index += [index]\n",
    "                                mislablled_identified_seq_types += [identified_seq_type[i]]\n",
    "\n",
    "                ### if two found rectangles overlap with a single motif then remove the smallest one \n",
    "                new_overlap_proportion, new_implanted_index,new_actually_identified_seq_types = remove_overlapping_rectangles(overlap_proportion, implanted_index,actually_identified_seq_types)\n",
    "\n",
    "                # remove not very overlapping ones\n",
    "                overlap_mask = np.array(new_overlap_proportion) > overlap_min_threshold\n",
    "                filtered_overlaps = np.array(new_overlap_proportion)[overlap_mask]\n",
    "\n",
    "                PPseq_correct_identified_seq_types = np.array(new_actually_identified_seq_types)[overlap_mask]\n",
    "\n",
    "                proportion_found = len(filtered_overlaps) / len(all_implanted_rectangles)\n",
    "                print(proportion_found)\n",
    "                true_seqs_found = len(filtered_overlaps)\n",
    "\n",
    "                # having determined which implanted seqs have been found, now I identify whihc ones have not been and see if these were founded by another seq\n",
    "                missing_elements = find_missing_elements( list(np.linspace(0,len(all_implanted_rectangles)-1,len(all_implanted_rectangles)).astype(int)),list(np.array(new_implanted_index)[overlap_mask]))\n",
    "\n",
    "                ## filter the mislablled one sin teh same way as the lablled ones:\n",
    "\n",
    "                ### if two found rectangles overlap with a single motif then remove the smallest one \n",
    "                mislablled_new_overlap_proportion, mislablled_new_implanted_index, new_mislablled_identified_seq_types = remove_overlapping_rectangles(mislablled_overlap_proportion, mislablled_implanted_index,mislablled_identified_seq_types)\n",
    "\n",
    "                # remove not very overlapping ones\n",
    "                overlap_mask = np.array(mislablled_new_overlap_proportion) > overlap_min_threshold\n",
    "                filtered_overlaps = np.array(mislablled_new_overlap_proportion)[overlap_mask]\n",
    "\n",
    "                PPseq_misidentified_seq_types = np.array(new_mislablled_identified_seq_types)[overlap_mask]\n",
    "\n",
    "                # if missing element is in filtered mislablled data then add it to mislablled total\n",
    "                total_mislablled = 0\n",
    "                mislablled_type = []\n",
    "                for item in missing_elements:\n",
    "                    if item in np.array(mislablled_new_implanted_index)[overlap_mask]:\n",
    "                        total_mislablled +=1\n",
    "                        mislablled_type += [PPseq_misidentified_seq_types[np.where(np.array(mislablled_new_implanted_index)[overlap_mask] == item)].astype(int)[0]]\n",
    "\n",
    "                # calculate seqs found over expected\n",
    "                seqs_found_over_implnated = []\n",
    "                seq_type_implnated = []\n",
    "                for item in np.unique(np.array(implanted_seq_types)):\n",
    "                    seq_type_implnated += [item]\n",
    "                    seqs_found_over_implnated += [(list(PPseq_correct_identified_seq_types).count(item) + mislablled_type.count(item)) / implanted_seq_types.count(item)]\n",
    "\n",
    "\n",
    "                print ('mislablled:')\n",
    "                all_found = total_mislablled + true_seqs_found\n",
    "                if all_found == 0:\n",
    "                    proportion_mislabeled = 0\n",
    "                else:\n",
    "                    proportion_mislabeled = total_mislablled/all_found\n",
    "                print(proportion_mislabeled)\n",
    "\n",
    "            else:\n",
    "                print('special case for blank test or no seqs')\n",
    "                if len(seq_spike_occurance) == 0:\n",
    "                    seqs_present = 0\n",
    "                else:\n",
    "                    for i in range(len(seq_spike_occurance)):\n",
    "                        current_seq = seqs[i]\n",
    "                        seq_spike_count = seq_spike_occurance[i]\n",
    "                        # find seq start and end, defined by whetehr there were spikes or not \n",
    "                        groups, peaks = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "                        if len(groups) == 0: \n",
    "                            seqs_present = 0\n",
    "                        else:\n",
    "                            seqs_present = 0\n",
    "                            for group in groups:\n",
    "                                timeframe = [(group[0]*bin_size),(group[-1]*bin_size)]\n",
    "                                mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "                                spikes_present = list(spikes_df[mask].sequence_type_adjusted.values.astype(int))\n",
    "                                if spikes_present.count(current_seq) > min_spikes_threshold:\n",
    "                                    seqs_present += 1\n",
    "\n",
    "                proportion_found = seqs_present \n",
    "                \n",
    "            # ------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "            #save stuff out!\n",
    "            #save out origional implantation into this folder\n",
    "            implanted_data.to_pickle(PP_save_path + '/' + session_name )\n",
    "\n",
    "            new_df = pd.DataFrame(\n",
    "            {'mouse_implant_recording' : [mouse_session_recording],\n",
    "            'type' : [session_name],\n",
    "            'proportion_found' : [proportion_found],\n",
    "            'proportion_mislabeled': [proportion_mislabeled],\n",
    "            'seqs_implanted':[seq_type_implnated],\n",
    "            'seqs_found_over_implanted':[seqs_found_over_implnated]})\n",
    "\n",
    "            #save this out\n",
    "            new_df.to_csv(PP_save_path + \"\\proportion_found.csv\")\n",
    "\n",
    "            #add to overall and save this out \n",
    "            output_df = pd.read_csv(output_data_file_path,index_col=0)\n",
    "            output_df = output_df.reset_index(drop =True)\n",
    "            output_df = output_df.append(new_df)\n",
    "            output_df.to_csv(output_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7503a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mislablled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "eeed6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mislablled + true_seqs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b2d60543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cf95b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mislablled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "46c11a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317d135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd60e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002f203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fd614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f8212df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_spike_occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(seq_spike_occurance)):\n",
    "    current_seq = seqs[i]\n",
    "    seq_spike_count = seq_spike_occurance[i]\n",
    "    # find seq start and end, defined by whetehr there were spikes or not \n",
    "    groups, peaks = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "    if len(groups) == 0: \n",
    "        seqs_present = 0\n",
    "    else:\n",
    "        seqs_present = 0\n",
    "        for group in groups:\n",
    "            timeframe = [(group[0]*bin_size),(group[-1]*bin_size)]\n",
    "            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "            spikes_present = list(spikes_df[mask].sequence_type_adjusted.values.astype(int))\n",
    "            if spikes_present.count(current_seq) > min_spikes_threshold:\n",
    "                seqs_present += 1\n",
    "\n",
    "proportion_found = seqs_present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616480eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4828fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "76b8b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_spike_occurance = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5bd17784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"            else:\\n\",\n",
    "\"                print('special case for blank test or no seqs')\\n\",\n",
    "\"\\n\",\n",
    "\"                for i in range(len(seq_spike_occurance)):\\n\",\n",
    "\"                    current_seq = seqs[i]\\n\",\n",
    "\"                    seq_spike_count = seq_spike_occurance[i]\\n\",\n",
    "\"                    # find seq start and end, defined by whetehr there were spikes or not \\n\",\n",
    "\"                    groups, peaks = return_inds_for_seq_groups(seq_spike_count)\\n\",\n",
    "\"\\n\",\n",
    "\"                    if len(groups) == 0: \\n\",\n",
    "\"                        seqs_present = 0\\n\",\n",
    "\"                    else:\\n\",\n",
    "\"                        seqs_present = 0\\n\",\n",
    "\"                        for group in groups:\\n\",\n",
    "                            timeframe = [(group[0]*bin_size),(group[-1]*bin_size)]\\n\",\n",
    "                            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\\n\",\n",
    "                            spikes_present = list(spikes_df[mask].sequence_type_adjusted.values.astype(int))\\n\",\n",
    "                            if spikes_present.count(current_seq) > min_spikes_threshold:\\n\",\n",
    "                                seqs_present += 1\\n\",\n",
    "          proportion_found = seqs_present \\n\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79307b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### need to add in, what it finds. ie. number of seq 1 found vs expected etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "86c4df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0188679245283019, 1.0, 0.9767441860465116, 0.9245283018867925]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_type_implnated\n",
    "seqs_found_over_implnated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "92f074b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e03be47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0188679245283019, 1.0, 0.9767441860465116, 0.9245283018867925]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_found_over_implnated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "528572aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_type_implnated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b5873048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 1]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(PPseq_correct_identified_seq_types).count(1) + mislablled_type.count(1)) / implanted_seq_types.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "775a3d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 3., 5., 3., 3., 3., 5., 3., 5., 5., 3., 3., 5., 3., 5., 3.,\n",
       "       3., 3., 5., 3., 5., 3., 5., 6., 4., 5., 4., 4., 4., 4., 4., 6., 4.,\n",
       "       5., 5., 4., 4., 6., 4., 4., 6., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
       "       4., 4., 6., 6., 6., 4., 4., 6., 2., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 3., 3., 3., 4., 2., 4., 3., 2., 4., 2., 4., 3., 3., 3., 4., 4.,\n",
       "       2., 3., 4., 2., 2., 3., 3., 4., 4., 3., 3., 4., 3., 2.])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPseq_misidentified_seq_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb6da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc175d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 206 but corresponding boolean dimension is 195",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_actually_identified_seq_types\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43moverlap_mask\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 206 but corresponding boolean dimension is 195"
     ]
    }
   ],
   "source": [
    "np.array(new_actually_identified_seq_types)[overlap_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f59125d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overlap_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21bd7f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_actually_identified_seq_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c602e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mislablled_identified_seq_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3de03a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11eee069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identified_seq_type[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d15198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implanted_seq_types[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de1007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77bb512e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actually_identified_seq_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ccafc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 33,\n",
       " 33,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 98,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 114,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 120,\n",
       " 126,\n",
       " 128,\n",
       " 129,\n",
       " 136,\n",
       " 142,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 147,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 151,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 154,\n",
       " 155,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 158,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 179,\n",
       " 180,\n",
       " 182,\n",
       " 182,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 186,\n",
       " 188,\n",
       " 189,\n",
       " 189,\n",
       " 190,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 193,\n",
       " 194,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 199]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislablled_implanted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "797ec0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00152430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5199c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number of false positives: ie. times it mislabelled an implanted seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739389b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d23ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "30f022c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\ppseq_analysis\\\\synthetic_data\\\\\\\\\\\\149_1_1_proportions_found.csv'"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "4f2e043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mouse_implant_recording</th>\n",
       "      <th>type</th>\n",
       "      <th>proportion_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149_1_1</td>\n",
       "      <td>dropped_spikes_insertion_99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mouse_implant_recording                         type  proportion_found\n",
       "0                 149_1_1  dropped_spikes_insertion_99               0.0"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "3339e5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\ppseq_analysis\\\\synthetic_data\\\\\\\\\\\\149_1_1_proportions_found.csv'"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "9804c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mouse_implant_recording</th>\n",
       "      <th>type</th>\n",
       "      <th>proportion_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_100</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_20</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_300</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_400</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_40</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_500</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_80</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>blank_test</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_10</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_20</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_30</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_40</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_50</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_60</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_70</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_80</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_85</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_90</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>dropped_spikes_insertion_95</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>standard_insertion</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_100</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_10</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_20</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_30</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_40</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_50</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_60</td>\n",
       "      <td>0.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_70</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_80</td>\n",
       "      <td>0.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>swapped_spikes_insertion_90</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion0.03</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion0.1</td>\n",
       "      <td>0.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion0.2</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion0.5</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion20</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion2</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>warp_insertion5</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148_2_2</td>\n",
       "      <td>background_noise_insertion_200</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149_1_1</td>\n",
       "      <td>dropped_spikes_insertion_99</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mouse_implant_recording                            type  proportion_found\n",
       "0                 148_2_2  background_noise_insertion_100             0.855\n",
       "0                 148_2_2   background_noise_insertion_20             0.960\n",
       "0                 148_2_2  background_noise_insertion_300             0.750\n",
       "0                 148_2_2  background_noise_insertion_400             0.740\n",
       "0                 148_2_2   background_noise_insertion_40             0.980\n",
       "0                 148_2_2  background_noise_insertion_500             0.680\n",
       "0                 148_2_2   background_noise_insertion_80             0.840\n",
       "0                 148_2_2                      blank_test             1.000\n",
       "0                 148_2_2     dropped_spikes_insertion_10             0.970\n",
       "0                 148_2_2     dropped_spikes_insertion_20             0.965\n",
       "0                 148_2_2     dropped_spikes_insertion_30             0.960\n",
       "0                 148_2_2     dropped_spikes_insertion_40             0.935\n",
       "0                 148_2_2     dropped_spikes_insertion_50             0.865\n",
       "0                 148_2_2     dropped_spikes_insertion_60             0.700\n",
       "0                 148_2_2     dropped_spikes_insertion_70             0.400\n",
       "0                 148_2_2     dropped_spikes_insertion_80             0.140\n",
       "0                 148_2_2     dropped_spikes_insertion_85             0.050\n",
       "0                 148_2_2     dropped_spikes_insertion_90             0.005\n",
       "0                 148_2_2     dropped_spikes_insertion_95             0.000\n",
       "0                 148_2_2              standard_insertion             0.730\n",
       "0                 148_2_2    swapped_spikes_insertion_100             0.270\n",
       "0                 148_2_2     swapped_spikes_insertion_10             0.960\n",
       "0                 148_2_2     swapped_spikes_insertion_20             0.970\n",
       "0                 148_2_2     swapped_spikes_insertion_30             0.955\n",
       "0                 148_2_2     swapped_spikes_insertion_40             0.795\n",
       "0                 148_2_2     swapped_spikes_insertion_50             0.895\n",
       "0                 148_2_2     swapped_spikes_insertion_60             0.595\n",
       "0                 148_2_2     swapped_spikes_insertion_70             0.575\n",
       "0                 148_2_2     swapped_spikes_insertion_80             0.490\n",
       "0                 148_2_2     swapped_spikes_insertion_90             0.430\n",
       "0                 148_2_2              warp_insertion0.03             0.515\n",
       "0                 148_2_2               warp_insertion0.1             0.635\n",
       "0                 148_2_2               warp_insertion0.2             0.725\n",
       "0                 148_2_2               warp_insertion0.5             0.715\n",
       "0                 148_2_2                warp_insertion10             0.000\n",
       "0                 148_2_2                warp_insertion20             0.000\n",
       "0                 148_2_2                 warp_insertion2             0.700\n",
       "0                 148_2_2                warp_insertion30             1.000\n",
       "0                 148_2_2                 warp_insertion5             0.800\n",
       "0                 148_2_2  background_noise_insertion_200             0.745\n",
       "0                 149_1_1     dropped_spikes_insertion_99             0.000"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7092b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
