{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "923958ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Make notebook wider:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "367fd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns;\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_H5_bodypart(tracking_path,video_type, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'task' in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "    \n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "    \n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "  \n",
    "def load_H5_ports(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'port' in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "    \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "def clean_and_interpolate(head_centre,threshold):\n",
    "    bad_confidence_inds = np.where(head_centre.likelihood.values<threshold)[0]\n",
    "    newx = head_centre.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = head_centre.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    head_centre['interped_x'] = interped_x\n",
    "    head_centre['interped_y'] = interped_y\n",
    "    \n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                #print('interp_start_index: ', interp_start_index)\n",
    "                #print('interp_start_value: ', value_before)\n",
    "                #print('')\n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                    #print('interp_end_index: ', interp_end_index)\n",
    "                    #print('interp_end_value: ', value_after)\n",
    "                    #print('')\n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                    #print('interp_diff_index is:', interp_diff_index)\n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        #print('interp_index is:', interp_index)\n",
    "                        #print('new_value should be:', new_values[x])\n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "#     print('function exiting')\n",
    "    return(coords_list)\n",
    "\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def load_in_paths(pp_file, PP_PATH, DAT_PATH):\n",
    "    mir = '_'.join(pp_file.split('_')[0:3])\n",
    "    print(str(run_index+1) + '/' + str(len(os.listdir(PP_PATH))-1) + '-------------------------------------------------------------------------')\n",
    "    print(pp_file)\n",
    "    mouse_session_recording = pp_file.split('_')[0] + '_' + pp_file.split('_')[1] + '_' + pp_file.split('_')[2] \n",
    "    skip = False\n",
    "    for item in ignore_list:\n",
    "        if item == mouse_session_recording:\n",
    "            skip = True\n",
    "\n",
    "    save_path = PP_PATH + pp_file + '\\\\analysis_output\\\\'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    ## set dat_path:\n",
    "    for file_ in os.listdir(DAT_PATH):\n",
    "        if mouse_session_recording.split('_')[0] in file_:\n",
    "            if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                dat_path = os.path.join(DAT_PATH,file_)\n",
    "    for recording in os.listdir(os.path.join(DAT_PATH,dat_path)):\n",
    "        if recording.split('_')[0][9::] == mouse_session_recording.split('_')[-1]:\n",
    "            dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "    # set tracking path\n",
    "    for file_ in os.listdir(dat_path + r\"\\video\\tracking\\\\\"):\n",
    "        if 'task' in file_:\n",
    "            if not 'clock' in file_:\n",
    "                tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\",file_) + '\\\\'\n",
    "   \n",
    "    return mir,mouse_session_recording,save_path,tracking_path,dat_path\n",
    "                        \n",
    "def load_PPSEQ_data(PP_PATH,pp_file,dat_path):\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    ## LOAD \n",
    "    print(\"LOADING PPSEQ DATA\")\n",
    "    print('\\n')\n",
    "    #The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\n",
    "    assignment_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\assigment_hist_frame.csv\")\n",
    "\n",
    "    # latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\n",
    "    latent_event_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\latent_event_hist.csv\")\n",
    "\n",
    "    # seq_type_log_proportions: log p of each type of sequence at each iteration\n",
    "    seq_type_log_proportions_df = pd.read_csv(PP_PATH + pp_file + r\"\\seq_type_log_proportions.csv\")\n",
    "\n",
    "    # neuron_responses.csv: iterations x neurons by 3(number of sequences). Each neuron has three parameters per sequence to describe how it is influenced by each sequence type. \n",
    "    # Each iteration these are resampled, therefore there are number of neurons by iterations by 3 by number of sequences of these numbers.\n",
    "    neuron_response_df = pd.read_csv(PP_PATH + pp_file + r\"\\neuron_response.csv\")\n",
    "\n",
    "\n",
    "    masking = False\n",
    "    for dat_files in os.listdir(PP_PATH + pp_file):\n",
    "        if 'unmasked_spikes' in dat_files:\n",
    "            masking = True\n",
    "            print('masking was used')\n",
    "\n",
    "    if masking == True:\n",
    "        #log_p_hist.csv: the history of the log_p of the model\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\test_log_p_hist.csv\")\n",
    "\n",
    "        unmasked_spikes_df = pd.read_csv(PP_PATH + pp_file + r\"\\unmasked_spikes.csv\")\n",
    "    else:\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\log_p_hist.csv\")\n",
    "\n",
    "        spikes_file = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + mouse_session_recording + '.txt'\n",
    "        neuron_ids, spike_times= [], []\n",
    "        with open(spikes_file) as f:\n",
    "            for (i, line) in enumerate(f.readlines()):\n",
    "                [neuron_id, spike_time] = line.split(' ', 1)\n",
    "                spike_time = eval(spike_time.split('\\n')[0])\n",
    "                neuron_id = eval(neuron_id.split('\\t')[0])\n",
    "                spike_times.append(spike_time)\n",
    "                neuron_ids.append(neuron_id)\n",
    "        unmasked_spikes_df = pd.DataFrame({'neuron':neuron_ids,'timestamp':spike_times}) \n",
    "\n",
    "    bkgd_log_proportions_array = pd.read_csv(PP_PATH + pp_file + r\"\\bkgd_log_proportions_array.csv\")\n",
    "\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open(PP_PATH + pp_file + r'\\config_file.json')\n",
    "    # returns JSON object as a dictionary\n",
    "    config = eval(json.load(f))\n",
    "    print(f'      done')\n",
    "\n",
    "    ## LOAD behaviour data\n",
    "    print('\\n')\n",
    "    print(\"LOADING BEHAV DATA\")\n",
    "\n",
    "    ## load in the timespan used for pppseq:\n",
    "    input_params_path = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + ('params_' + mouse_session_recording +'.json')\n",
    "    # Opening JSON file\n",
    "    f = open(input_params_path)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    input_config = json.load(f)\n",
    "    behav_time_interval_start = input_config['time_span']\n",
    "    print(f\"      A corresponding time span has been found. Time span set to {behav_time_interval_start}\")\n",
    "\n",
    "    ### load in data:\n",
    "    for sub_file in os.listdir(dat_path + '\\\\behav_sync\\\\'):\n",
    "        if 'task' in sub_file:\n",
    "            behav_sync_path = dat_path + '\\\\behav_sync\\\\' + sub_file +'\\\\'\n",
    "    behav_sync = pd.read_csv(behav_sync_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "    transitions = pd.read_csv(behav_sync_path + 'Transition_data_sync.csv')\n",
    "\n",
    "    return assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start\n",
    "\n",
    "def plot_save_log_l_curve(log_p_hist_df,save_path):\n",
    "    # find 95% of growth value and when it crossed this\n",
    "    max_ = max(log_p_hist_df.x1)\n",
    "    min_ = min(log_p_hist_df.x1)\n",
    "    growth = max_ - min_\n",
    "    _prcntile =  max_ - (0.02 * growth)\n",
    "\n",
    "    ## model log likley hood curve\n",
    "    plt.plot(log_p_hist_df.x1)\n",
    "    plt.axhline(y=_prcntile, color='r', linestyle='--')\n",
    "\n",
    "    SaveFig('log_l_curve.png',save_path)\n",
    "    \n",
    "def plot_data_raster(behav_time_interval_start, spikes_df, neuron_index, colors, save_path):\n",
    "    # calculate interval timings and end points\n",
    "    interval_lengths = []\n",
    "    for interval in behav_time_interval_start:\n",
    "        interval_lengths += [np.diff(interval)[0]]\n",
    "    total_time = sum(interval_lengths)\n",
    "    interval_end_points = np.cumsum(interval_lengths)\n",
    "\n",
    "    # Plot sequences - basic\n",
    "    timeframe = [0, total_time]\n",
    "    mask = (spikes_df.timestamp > timeframe[0]) * (spikes_df.timestamp < timeframe[-1])\n",
    "\n",
    "    # Define neuron order\n",
    "    neuron_permute_loc = np.zeros(len(neuron_index))\n",
    "    for i in range(len(neuron_index)):\n",
    "        neuron_permute_loc[i] = int(list(neuron_index).index(i))\n",
    "    neuron_order = neuron_permute_loc[(spikes_df.neuron - 1).astype(int)]\n",
    "\n",
    "    # Plotting\n",
    "    fig, [ax, ax2] = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "    # Plot background in grey\n",
    "    background_keep_mask = (spikes_df[mask].sequence_type_adjusted < 0) | (spikes_df[mask].sequence_type_adjusted >= 7.0)\n",
    "    ax.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "               marker='o', s=40, linewidth=0, color='lightgrey', alpha=0.3)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_keep_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax2.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "                marker='o', s=40, linewidth=0, color=c_, alpha=0.3)\n",
    "    ax2.set_title('extra sequences and background only')\n",
    "\n",
    "    # Plot spikes without background\n",
    "    background_remove_mask = (spikes_df[mask].sequence_type_adjusted >= 0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 7.0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 8.0)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],\n",
    "               marker='o', s=40, linewidth=0, color=c_, alpha=1)\n",
    "    ax.set_title('held sequences in color and extra sequences + background in grey')\n",
    "\n",
    "    for end_p in interval_end_points:\n",
    "        ax.axvline(x=end_p, color='k')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    return interval_end_points,neuron_order\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def conactinate_nth_items(startlist):\n",
    "    concatinated_column_vectors = []\n",
    "    for c in range(len(max(startlist, key=len))):\n",
    "        column = []\n",
    "        for t in range(len(startlist)):\n",
    "            if c <= len(startlist[t])-1:\n",
    "                column = column + [startlist[t][c]]\n",
    "        concatinated_column_vectors.append(column)\n",
    "    return concatinated_column_vectors\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def split_list(numbers):\n",
    "    chunks = []\n",
    "    indices = []\n",
    "    current_chunk = []\n",
    "    current_indices = []\n",
    "    for i, num in enumerate(numbers):\n",
    "        if num == 0:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                indices.append(current_indices)\n",
    "                current_chunk = []\n",
    "                current_indices = []\n",
    "        else:\n",
    "            current_chunk.append(num)\n",
    "            current_indices.append(i)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        indices.append(current_indices)\n",
    "    return chunks, indices\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def catagorize_seqs(real_order,num_dominant_seqs,order):\n",
    "    \n",
    "    #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "    if not len(real_order) == num_dominant_seqs:\n",
    "        dominant = real_order[0:num_dominant_seqs]\n",
    "        other_ = real_order[num_dominant_seqs::]\n",
    "    else:\n",
    "        dominant = real_order\n",
    "        other_ = []\n",
    "\n",
    "    amounts = []\n",
    "    relative_amounts = []\n",
    "    pair_outcomes = []\n",
    "    pairs = []\n",
    "    for sequence in order:\n",
    "        ordered = 0\n",
    "        reverse = 0\n",
    "        repeat = 0\n",
    "        misordered = 0\n",
    "        non_to_task = 0\n",
    "        task_to_non = 0\n",
    "        other = 0\n",
    "        for index,element in enumerate(sequence[0:-1]):\n",
    "            pair = [element,sequence[index+1]]\n",
    "            outcome = (logic_machine_for_pair_catagorisation(pair,dominant,other_))\n",
    "            if outcome == 'ordered':\n",
    "                ordered +=1\n",
    "            elif outcome == 'reverse':\n",
    "                reverse +=1\n",
    "            elif outcome == 'repeat':\n",
    "                repeat +=1\n",
    "            elif outcome == 'misordered':\n",
    "                misordered +=1\n",
    "            elif outcome == 'task to other':\n",
    "                task_to_non +=1\n",
    "            elif outcome == 'other to task':\n",
    "                non_to_task +=1\n",
    "            elif outcome == 'other':\n",
    "                other +=1\n",
    "            pairs += [pair]\n",
    "            pair_outcomes += [[outcome]]\n",
    "        pairs += [['None']]\n",
    "        pair_outcomes += [['None']]\n",
    "\n",
    "        relative_amounts += [list(np.array([ordered,reverse,repeat,misordered,non_to_task,task_to_non,other])/(len(sequence)-1))]\n",
    "        amounts += [[ordered,reverse,repeat,misordered,non_to_task,task_to_non,other]]\n",
    "        \n",
    "    return relative_amounts, amounts,pair_outcomes,pairs\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_inds_for_seq_groups(lst):\n",
    "    groups = []\n",
    "    new = True\n",
    "    for ind,item in enumerate(lst):\n",
    "        if new:\n",
    "            if item > 0:\n",
    "                start = ind\n",
    "                new = False\n",
    "        else:\n",
    "            if item == 0:\n",
    "                end = ind-1\n",
    "                groups.append((start, end))\n",
    "                new = True\n",
    "    return groups\n",
    "\n",
    "\n",
    "def plot_event_transitions(event_transitions,axes,title):\n",
    "    transitions = list(event_transitions.keys())\n",
    "    counts = list(event_transitions.values())\n",
    "\n",
    "    # Create a bar chart\n",
    "    axes.bar(range(len(transitions)), counts,alpha = 0.5)\n",
    "\n",
    "    # Set x-axis labels\n",
    "    labels = [str(transition) for transition in transitions]\n",
    "    axes.set_xticks(range(len(transitions)), labels, rotation=90)\n",
    "\n",
    "    # Set y-axis label\n",
    "    axes.set_ylabel('Normalized Occurrences %')\n",
    "\n",
    "    # Add title\n",
    "    axes.set_title('Event Transitions' + title)\n",
    "\n",
    "def count_event_transitions(event_list):\n",
    "    \n",
    "    ### change code so that it sets each dictionary with these pairs a sa starting point! \n",
    "    from itertools import product\n",
    "    numbers = range(1, 7)  # Numbers 1 to 6a\n",
    "    all_possible_pairs = list(product(numbers, repeat=2))\n",
    "\n",
    "    transitions = {key: 0 for key in all_possible_pairs}  # Create an empty dictionary with the defined keys\n",
    "    \n",
    "    total_transitions = len(event_list) - 1\n",
    "    \n",
    "    for i in range(total_transitions):\n",
    "        current_event = event_list[i]\n",
    "        next_event = event_list[i + 1]\n",
    "        transition = (current_event, next_event)\n",
    "        if transition in transitions:\n",
    "            transitions[transition] += 1\n",
    "        else:\n",
    "            transitions[transition] = 1\n",
    "    \n",
    "    normalized_transitions = {}\n",
    "    for transition, count in transitions.items():\n",
    "        normalized_count = count / total_transitions * 100\n",
    "        normalized_transitions[transition] = normalized_count\n",
    "    \n",
    "    return normalized_transitions\n",
    "\n",
    "\n",
    "def normalize_counts_to_percentages(events_dict):\n",
    "    total_count = sum(events_dict.values())\n",
    "    normalized_dict = {}\n",
    "\n",
    "    for event, count in events_dict.items():\n",
    "        percentage = (count / total_count) * 100\n",
    "        normalized_dict[event] = percentage\n",
    "\n",
    "    return normalized_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494adaef",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecf01461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/37-------------------------------------------------------------------------\n",
      "136_1_3_run_1007023_2048\n",
      "LOADING PPSEQ DATA\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m mir,mouse_session_recording,save_path,tracking_path,dat_path \u001b[38;5;241m=\u001b[39m load_in_paths(pp_file, PP_PATH, DAT_PATH)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m## load in PPseq output data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start \u001b[38;5;241m=\u001b[39m \u001b[43mload_PPSEQ_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdat_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# plot out log l curve \u001b[39;00m\n\u001b[0;32m     22\u001b[0m plot_save_log_l_curve(log_p_hist_df,save_path)\n",
      "Cell \u001b[1;32mIn[97], line 203\u001b[0m, in \u001b[0;36mload_PPSEQ_data\u001b[1;34m(PP_PATH, pp_file, dat_path)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m#The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m assignment_history_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPP_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpp_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43massigment_hist_frame.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\u001b[39;00m\n\u001b[0;32m    206\u001b[0m latent_event_history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(PP_PATH \u001b[38;5;241m+\u001b[39m pp_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlatent_event_hist.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ignore_list= []\n",
    "PP_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "DAT_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "# PP_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\output_data\\striatum\\New_post_sleep_shuffle\\\\\"\n",
    "# PP_PATH = \"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\output_data\\old_data_filter_method\\striatum\\Medium_post_sleep\\8_seq\\\\\"\n",
    "\n",
    "######################################################################################################################################################################################################################\n",
    "# load in data, extract, filter PPseq spikes etc.\n",
    "# main filtering here:\n",
    "# backgroudn confidence - spike had to be classified as a seq type 75% of the time across the last 50 iterations to be kept.   \n",
    "######################################################################################################################################################################################################################\n",
    "\n",
    "for run_index,pp_file in enumerate(os.listdir(PP_PATH)):\n",
    "    \n",
    "    if run_index >-1 and 'run' in pp_file:\n",
    "        # load in paths for that specific mouse and recording\n",
    "        mir,mouse_session_recording,save_path,tracking_path,dat_path = load_in_paths(pp_file, PP_PATH, DAT_PATH)\n",
    "        \n",
    "        ## load in PPseq output data\n",
    "        assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start = load_PPSEQ_data(PP_PATH,pp_file,dat_path)\n",
    "        # plot out log l curve \n",
    "        plot_save_log_l_curve(log_p_hist_df,save_path)\n",
    "        \n",
    "    # ---filter_across_itterations---------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        # Initialize an empty df to store the result\n",
    "        seq_types_df = pd.DataFrame()\n",
    "        # Iterate through the range\n",
    "        # for iteration_ in tqdm(range(400, 500)):\n",
    "        for iteration_ in tqdm(range(250, 300)):\n",
    "            # Extract the relevant column from the assignment history dataframe\n",
    "            assignment_history_df_split = assignment_history_df[str(list(assignment_history_df)[iteration_])]\n",
    "            # Get the index of the -1 split markers in the latent event history dataframe\n",
    "            end_markers = latent_event_history_df.loc[latent_event_history_df['seq_type'] == -1.0].index\n",
    "            # Extract the relevant portion of the latent event history dataframe\n",
    "            latent_event_history_df_split =  latent_event_history_df[end_markers[iteration_-1]:end_markers[iteration_]]\n",
    "            # Create a dictionary from the dataframe for faster lookups\n",
    "            df_dict = latent_event_history_df_split.set_index('assignment_id')['seq_type'].to_dict()\n",
    "            # Match the sequence ID to the sequence type\n",
    "            seq_type = find_corresponding(assignment_history_df_split)\n",
    "            # Append the result to the df\n",
    "            seq_types_df[str(iteration_+1)] = seq_type\n",
    "        proportion = []\n",
    "        seq_type = []\n",
    "        for index in tqdm(range(len(seq_types_df))):\n",
    "            row = seq_types_df.loc[index]\n",
    "            seq_type += [statistics.mode(row)] \n",
    "            proportion += [np.count_nonzero(row == statistics.mode(row)) / len(row)]\n",
    "        # add seq type to dataframe\n",
    "        unmasked_spikes_df['sequence_type'] = seq_type\n",
    "        # add seq type to dataframe\n",
    "        unmasked_spikes_df['seq_confidence'] = proportion\n",
    "        \n",
    "        # ## filter for background confidence :-------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        thresh = max(proportion) *.6 ### \n",
    "        plt.plot(np.sort(proportion)[::-1])\n",
    "        plt.axhline(y = thresh, color = 'r', linestyle = '-')\n",
    "        unmasked_spikes_df['sequence_type_adjusted'] = seq_type\n",
    "        unmasked_spikes_df.sequence_type_adjusted[np.where(unmasked_spikes_df.seq_confidence < thresh)[0]] = -1\n",
    "        SaveFig('filtering_curve.png',save_path)\n",
    "        \n",
    "        ## load in colors and order from awake data -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "        for file_ in os.listdir(awake_PP_path):\n",
    "            if mouse_session_recording in file_:\n",
    "                awake_file = file_\n",
    "        ordered_preferred_type = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'ordered_preferred_type')\n",
    "        neuron_index = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'neuron_index')\n",
    "        colors = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'colors')\n",
    "        spikes_df = unmasked_spikes_df\n",
    "        colors += ['pink','lightblue', 'k'] \n",
    "        \n",
    "        ############### plot simple rasters ------------------------------\n",
    "        interval_end_points,neuron_order = plot_data_raster(behav_time_interval_start, spikes_df, neuron_index, colors, 'all_data_raster.png')\n",
    "\n",
    "\n",
    "        ######################################################################################################################################################################################################################\n",
    "        # filter replays \n",
    "        # spikes for each type are binned into 40ms time bins.  ## used to be 20ms - for phd thesis\n",
    "        # if spikes occur continuously across bins then this is classified as a single contiguous replay event \n",
    "\n",
    "        # if at least 5 spikes and 3 neurons were involved its called a replay.\n",
    "\n",
    "        ######################################################################################################################################################################################################################    \n",
    "\n",
    "        min_spikes_filter = 5\n",
    "        min_neurons_involved_filter = 3\n",
    "        bin_size = 0.04\n",
    "\n",
    "        chunk_paths = []\n",
    "        # run for each time chunk: \n",
    "        for index_,interval_start in enumerate([0] + list(interval_end_points)[0:-1]):\n",
    "\n",
    "            ## define path for this chunk\n",
    "            chunk_path = save_path + 'chunk' + str(index_+1)+'_' + str(behav_time_interval_start[index_][0]) + 'to' + str(behav_time_interval_start[index_][1]) + '\\\\'\n",
    "            chunk_paths += [chunk_path]\n",
    "            if not os.path.isdir(chunk_path):\n",
    "                os.mkdir(chunk_path)\n",
    "\n",
    "            ## save out time interval for current chunk:\n",
    "            np.save(chunk_path+ 'chunk_time_interval.npy',np.array(behav_time_interval_start[index_]))\n",
    "\n",
    "            timeframe = [interval_start,interval_end_points[index_]-1]\n",
    "            total_time = np.diff(timeframe)[0]+1\n",
    "            #mask\n",
    "            # spikemask for plotting\n",
    "            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "\n",
    "            fig, [ax1,ax2] = plt.subplots(2, 1,figsize=(35, 15))\n",
    "\n",
    "            # plot spikes without background\n",
    "            background_remove_mask = (spikes_df[mask].sequence_type_adjusted > -1) * (spikes_df[mask].sequence_type_adjusted < 7)\n",
    "            c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "            ax1.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "\n",
    "            # define chunk data:\n",
    "            chunk_mask = (spikes_df.timestamp > interval_start) * (spikes_df.timestamp < interval_end_points[index_])\n",
    "            chunk_df = spikes_df[chunk_mask].copy()\n",
    "            chunk_df = chunk_df.reset_index(drop=True)\n",
    "\n",
    "            if not run_index == 16:\n",
    "                #save out chunk data \n",
    "                chunk_df.to_csv(chunk_path + 'unfiltered_spikes_data.csv', index=False)\n",
    "\n",
    "            ### bin the spiking for each seq type\n",
    "            seqs = np.unique(chunk_df.sequence_type_adjusted)\n",
    "            seq_spikes = []\n",
    "            seq_neurons = []\n",
    "            for seq_type_ in seqs:  \n",
    "                seq_spikes += [chunk_df.timestamp[np.where(chunk_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "                seq_neurons += [chunk_df.neuron[np.where(chunk_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "            binned_seq_r_events = []\n",
    "            for spikes_ in seq_spikes:\n",
    "                # Use the numpy.histogram function to bin the data\n",
    "                hist, bins = np.histogram(spikes_, bins=np.arange(interval_start, interval_end_points[index_], bin_size))\n",
    "                binned_seq_r_events += [list(hist)]\n",
    "            strt_ = int(timeframe[0]/bin_size)\n",
    "            end_ = int(timeframe[1]/bin_size)   \n",
    "\n",
    "            # loop over smoothed data for eeahc sew and create cluster chunks from this: \n",
    "            r_start_ = []\n",
    "            r_end_ = []\n",
    "            r_seq_type = []\n",
    "            for _index_,sequence_type in enumerate(seqs):\n",
    "                sequence_type = int(sequence_type)\n",
    "                # if seq type not background or other new types:\n",
    "                if sequence_type > 0 and sequence_type <= 6:\n",
    "                    print(sequence_type)\n",
    "\n",
    "                    # smooth over binned spikes:\n",
    "                    smoothed_binned_spikes = convolve_movmean(binned_seq_r_events[_index_],2)\n",
    "\n",
    "                    # plot smoothed counts for this sequence \n",
    "                    time_bins = np.arange(timeframe[0],timeframe[0]+np.diff(timeframe)+1,bin_size)\n",
    "                    ax2.plot(time_bins[0:-1],smoothed_binned_spikes, c = colors[sequence_type])\n",
    "                    ax2.sharex(ax1)\n",
    "\n",
    "                    ## split smoothed replay into single events\n",
    "                    replay_chunks,indices = split_list(list(smoothed_binned_spikes))\n",
    "\n",
    "                    for index,chunk in enumerate(replay_chunks):\n",
    "                        # start and end of chunk event:\n",
    "                        r_seq_type += [sequence_type]\n",
    "                        r_start_ += [time_bins[indices[index][0]]]\n",
    "                        r_end_ += [time_bins[indices[index][-1]]]\n",
    "\n",
    "            ## perform filtering on cluster chunks\n",
    "            #filter1 number of spikes\n",
    "            #filter2 number of neurons \n",
    "            filtered_cluster_sequence_type = []\n",
    "            filtered_cluster_timestamps = []\n",
    "            filtered_num_spikes = []\n",
    "            filtered_num_neurons = []\n",
    "            filtered_first_spike_time = []\n",
    "            filtered_last_spike_time = []\n",
    "            filtered_cluster_neurons = []\n",
    "            event_length = []\n",
    "            cluster_neuron_order = []\n",
    "            for i in range(len(r_seq_type)):\n",
    "                r_event_df = chunk_df[(chunk_df.timestamp >= r_start_[i]) * (chunk_df.timestamp <= r_end_[i])].copy()\n",
    "                neuron_orders = neuron_order[chunk_mask][(chunk_df.timestamp >= r_start_[i]) * (chunk_df.timestamp <= r_end_[i])]\n",
    "                # only neurons of the seq type\n",
    "                neuron_orders = neuron_orders[r_event_df.sequence_type_adjusted == r_seq_type[i]]\n",
    "                r_event_df = r_event_df[r_event_df.sequence_type_adjusted == r_seq_type[i]]\n",
    "                if len(r_event_df) > 0:\n",
    "                    num_spikes = len(r_event_df.sequence_type_adjusted)\n",
    "                    num_neurons = len(r_event_df.neuron.unique())\n",
    "                    first_spike = min(r_event_df.timestamp)\n",
    "                    last_spike = max(r_event_df.timestamp)\n",
    "\n",
    "                    if num_spikes >= min_spikes_filter:\n",
    "                        if num_neurons >= min_neurons_involved_filter:\n",
    "                            ax1.axvspan(first_spike,last_spike, color=colors[r_seq_type[i]], alpha=0.5)\n",
    "                            ## save replay clusters out!\n",
    "                            filtered_cluster_sequence_type += [r_seq_type[i]]\n",
    "                            filtered_cluster_timestamps += [list(r_event_df.timestamp.values)]\n",
    "                            filtered_cluster_neurons += [list(r_event_df.neuron.values)]\n",
    "                            filtered_num_spikes += [num_spikes]\n",
    "                            filtered_num_neurons += [last_spike]\n",
    "                            filtered_first_spike_time += [first_spike]\n",
    "                            filtered_last_spike_time += [last_spike]\n",
    "                            event_length += [last_spike-first_spike]\n",
    "                            cluster_neuron_order += [neuron_orders]\n",
    "\n",
    "            filtered_r_clusters_df = pd.DataFrame({'cluster_seq_type':filtered_cluster_sequence_type, 'num_spikes':filtered_num_spikes,  'num_neurons': filtered_num_neurons,'first_spike_time':filtered_first_spike_time,'event_length':event_length,'last_spike_time':filtered_last_spike_time, 'cluster_spike_times':filtered_cluster_timestamps,'cluster_neurons':filtered_cluster_neurons,'spike_plotting_order': cluster_neuron_order})\n",
    "            filtered_r_clusters_df.to_csv(chunk_path + 'filtered_replay_clusters_df.csv', index=False)    \n",
    "\n",
    "            ax1.set_xlim(timeframe[0]+140,timeframe[0]+150)\n",
    "            ax2.set_xlim(timeframe[0]+140,timeframe[0]+150)\n",
    "\n",
    "\n",
    "            SaveFig('zoomed_data_filtering_chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "            \n",
    "            \n",
    "            ######################################################################################################################################################################################################################\n",
    "            # cluster these events into coactive chunks\n",
    "            # if events are within 200ms of each other then they are clustered together\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            #cluster events\n",
    "            start_times = filtered_r_clusters_df.first_spike_time.values\n",
    "            end_times = filtered_r_clusters_df.last_spike_time.values\n",
    "            event_proximity_filter =  0.2 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "\n",
    "            clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "\n",
    "            spike_times = [item for sublist in filtered_r_clusters_df.cluster_spike_times.values for item in sublist]    \n",
    "            neuron_orders = [item for sublist in filtered_r_clusters_df.spike_plotting_order.values for item in sublist]    \n",
    "\n",
    "            cst = []\n",
    "            for index,item in enumerate(filtered_r_clusters_df.cluster_spike_times):\n",
    "                cst += len(item)*[filtered_r_clusters_df.cluster_seq_type.values[index]]\n",
    "            c_ = np.array(colors)[np.array(cst)]\n",
    "            fig, [ax1,ax2] = plt.subplots(2, 1,figsize=(20, 10))\n",
    "\n",
    "            ax1.scatter(spike_times, neuron_orders,marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "            ax2.scatter(spike_times, neuron_orders,marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "            for i in range(len(start_times)):\n",
    "                ax1.axvspan(start_times[i],end_times[i],color = colors[filtered_r_clusters_df.cluster_seq_type.values[i]],alpha = 0.2)\n",
    "\n",
    "            for cluster in clustered_events:\n",
    "                starts = []\n",
    "                ends = []\n",
    "                for item in cluster:\n",
    "                    starts += [filtered_r_clusters_df.first_spike_time[item]]\n",
    "                    ends += [filtered_r_clusters_df.last_spike_time[item]]\n",
    "                ax2.axvspan(min(starts),max(ends),color = 'grey' ,alpha = 0.2)\n",
    "\n",
    "            ax1.set_title('single R events')\n",
    "            ax2.set_title('co-ative R event groups')\n",
    "            ax1.set_xlim(timeframe[0]+60,timeframe[0]+90)\n",
    "            ax2.set_xlim(timeframe[0]+60,timeframe[0]+90)\n",
    "\n",
    "            SaveFig('zoomed_r_event_co-occurance__chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            cluster_group = np.zeros(len(filtered_r_clusters_df))\n",
    "            for index,cluster in enumerate(clustered_events):\n",
    "                for item in cluster:\n",
    "                    cluster_group[item] = int(index)\n",
    "\n",
    "            # add this to df\n",
    "            filtered_r_clusters_df['coactive_cluster_group'] = cluster_group\n",
    "            # save this out \n",
    "            filtered_r_clusters_df.to_csv(chunk_path + 'filtered_replay_clusters_df.csv', index=False) \n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot single event length\n",
    "            # all single events (not just the ones that arent in coactive clusters)\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            fig, ax = plt.subplots(1, 1,figsize=(2, 7))\n",
    "            ax.plot(np.zeros(len(filtered_r_clusters_df.event_length)),filtered_r_clusters_df.event_length,'o',alpha = 0.4)\n",
    "\n",
    "            plt_df = pd.DataFrame({'x':['single event length']*len(filtered_r_clusters_df.event_length) ,'time': filtered_r_clusters_df.event_length})\n",
    "            ax=sns.boxplot( y = 'time', x = 'x', data = plt_df, color = 'blue', width = .2, zorder = 10,\\\n",
    "                        showcaps = True, boxprops = {'facecolor':'none', \"zorder\":10},\\\n",
    "                        showfliers=False, whiskerprops = {'linewidth':2, \"zorder\":10},\\\n",
    "                           saturation = 1, orient = 'v',ax = ax)\n",
    "\n",
    "\n",
    "            ax.plot(0,np.mean(filtered_r_clusters_df.event_length),'_',color = 'firebrick',markersize = 10)\n",
    "\n",
    "            SaveFig('single_R_event_lenght__chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot event rate over time in chunk\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            # List of event times\n",
    "            event_times = filtered_r_clusters_df.first_spike_time  # Add your event times here\n",
    "\n",
    "\n",
    "            bin_size = np.round((np.diff(timeframe)[[0]][0])/60)\n",
    "\n",
    "            # Calculate the number of events per minute\n",
    "            events_per_time, bins = np.histogram(event_times, bins=np.arange(timeframe[0],timeframe[0]+np.diff(timeframe)+1,bin_size))\n",
    "\n",
    "            # Calculate the mean events per minute\n",
    "            mean_events_per_time = np.mean(events_per_time)\n",
    "\n",
    "            # Create the figure and axes\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot the number of events per minute\n",
    "            ax.plot(bins[:-1], events_per_time, drawstyle='steps-post')\n",
    "\n",
    "            # Plot the vertical line representing the mean events per minute\n",
    "            ax.axhline(y=mean_events_per_time, color='r', linestyle='--', label='Mean')\n",
    "\n",
    "            # Set the x-axis label\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            # Set the y-axis label\n",
    "            ax.set_ylabel('Number of Events')\n",
    "            # Set the title\n",
    "            ax.set_title('Number of Events per min (approx)')\n",
    "\n",
    "            # Add a legend\n",
    "            ax.legend()\n",
    "\n",
    "            SaveFig('sinlge_R_events_over_time__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot frequency of coactive events by group \n",
    "            # save out these frequencies \n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "\n",
    "            group_sizes = []\n",
    "            for coactive_group in filtered_r_clusters_df.coactive_cluster_group.unique():\n",
    "                coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                group_sizes += [len(filtered_r_clusters_df[coactive_group_mask])]\n",
    "\n",
    "            # Counting occurrences of each number\n",
    "            counts = {}\n",
    "            for num in group_sizes:\n",
    "                if num in counts:\n",
    "                    counts[num] += 1\n",
    "                else:\n",
    "                    counts[num] = 1\n",
    "\n",
    "            # Extracting numbers and their frequencies as separate lists\n",
    "            x = list(counts.keys())\n",
    "            y = list(counts.values())\n",
    "\n",
    "            # Creating the bar chart\n",
    "            plt.bar(x, y)\n",
    "\n",
    "            # Adding labels and title\n",
    "            plt.xlabel('Numbers')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('total frequency of coactive event sizes ')\n",
    "\n",
    "            SaveFig('coactive_frequencies__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ### save out data:\n",
    "            if not run_index == 16:\n",
    "                chunk_df.to_csv(chunk_path + 'unfiltered_spikes_data.csv', index=False)\n",
    "\n",
    "            # Specify the file path where you want to save the dictionary\n",
    "            file_path = chunk_path + 'coactive_frequencies.json'\n",
    "            # Open the file in write mode and use json.dump to save the dictionary\n",
    "            with open(file_path, \"w\") as file:\n",
    "                json.dump(counts, file)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot motif frequenices over time for that chunk (1), total freqs by sequence (2) and task v non task (3)\n",
    "\n",
    "\n",
    "            # 1 #####################################################################################################################################################################################################################    \n",
    "\n",
    "            # for each group size, find these groups and find their times\n",
    "            all_group_event_times = {}\n",
    "            for group_size in np.unique(group_sizes):\n",
    "                group_event_times = []\n",
    "                np.where(np.array(group_sizes) == group_size)\n",
    "                groups_of_size_inds = np.where(np.array(group_sizes) == group_size)[0]\n",
    "                groups_of_size = list(filtered_r_clusters_df.coactive_cluster_group[groups_of_size_inds])\n",
    "                for coactive_group in groups_of_size:\n",
    "                    coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                    group_event_times += [min(filtered_r_clusters_df[coactive_group_mask].first_spike_time)]\n",
    "                all_group_event_times[group_size] = group_event_times\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            for item in all_group_event_times.keys():\n",
    "                ax.plot(all_group_event_times[item],np.ones(len(all_group_event_times[item]))*(item+1),'x')    \n",
    "            # for i,group_times in enumerate(all_group_event_times):\n",
    "            #     ax.plot(group_times,np.ones(len(group_times))*(i+1),'x')\n",
    "            # Set the x-axis label\n",
    "            ax.set_xlabel('coative event times (s)')\n",
    "            # Set the y-axis label\n",
    "            ax.set_ylabel('number of R events coactive together')\n",
    "\n",
    "            SaveFig('coactive_event_times__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # Specify the file path where you want to save the dictionary\n",
    "            file_path = chunk_path + 'coactive_event_times.pickle'\n",
    "            with open(file_path, 'wb') as handle:\n",
    "                pickle.dump(all_group_event_times, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # 2 ####################################################################################################################################################################################################\n",
    "\n",
    "            awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "\n",
    "            for index_,M_I_R in enumerate(os.listdir(awake_PP_path)):\n",
    "                if not M_I_R == 'not_suitable':\n",
    "                    mouse = '_'.join(M_I_R.split('_')[0:3])\n",
    "                    if mouse == mir:\n",
    "                        print(mouse)\n",
    "                        c_path = awake_PP_path + M_I_R + r\"\\analysis_output\\reordered_recolored\\\\\" \n",
    "\n",
    "            sequence_order_df = pd.read_csv(awake_PP_path+\"sequence_order.csv\")\n",
    "\n",
    "            import ast\n",
    "            seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == mir].dominant_task_seqs)\n",
    "            extended_list = seq_order.copy()  # Create a copy of the original list\n",
    "            for num in range(6):\n",
    "                if num not in extended_list:\n",
    "                    extended_list += [num]\n",
    "            extended_list = np.array(extended_list)\n",
    "\n",
    "            # Counting occurrences of each number\n",
    "            counts = {}\n",
    "            for item in range(1,7):\n",
    "                counts[item] = list(filtered_r_clusters_df.cluster_seq_type).count(item)\n",
    "\n",
    "            # Extracting numbers and their frequencies as separate lists\n",
    "            x = list(counts.keys())\n",
    "            y = list(counts.values())\n",
    "\n",
    "            # reorder into seq order:\n",
    "            x_ordered = np.array(x)[extended_list]\n",
    "            y_ordered = np.array(y)[extended_list]\n",
    "\n",
    "            # Creating the bar chart\n",
    "            plt.bar([0,1,2,3,4,5],y_ordered, color = np.array(colors)[extended_list+1])\n",
    "\n",
    "\n",
    "            # Customizing x-axis labels\n",
    "            custom_labels = np.array(x_ordered).astype(str)\n",
    "            for index,item in enumerate(seq_order):\n",
    "                for i,label in enumerate(custom_labels):\n",
    "                    if str(item+1) == label: \n",
    "                        custom_labels[i] = custom_labels[i] + ' *'\n",
    "                        break\n",
    "            plt.xticks([0,1,2,3,4,5], custom_labels)\n",
    "\n",
    "            # Adding labels and title\n",
    "            plt.xlabel('motif     *task related')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(' frequency of motfs ')\n",
    "\n",
    "            SaveFig('motif_frequencies__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # 3 ####################################################################################################################################################################################################\n",
    "            non_seqs = []\n",
    "            for i in range(6):\n",
    "                if not i in seq_order:\n",
    "                    non_seqs += [i]\n",
    "\n",
    "            fig, ax= plt.subplots(1, 1,figsize=(5, 5))\n",
    "            task_related_events = sum(np.array(y)[seq_order])\n",
    "            non_task_related_events = sum(np.array(y)[non_seqs])\n",
    "            ax.set_title('total_events')\n",
    "            ax.bar(['task related','non task related'],[task_related_events,non_task_related_events])\n",
    "\n",
    "            SaveFig('task-related_vs_non-task-related__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # save out this data: \n",
    "            np.save(chunk_path+'total_task_related_event.npy',task_related_events)\n",
    "            np.save(chunk_path+'total_nontask_related_event.npy',non_task_related_events)\n",
    "\n",
    "            np.save(chunk_path+'task_order_seqs.npy',seq_order)\n",
    "            np.save(chunk_path+'nontask_seqs.npy',non_seqs)\n",
    "\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot coaive_motif_transitions heatmap \n",
    "\n",
    "\n",
    "            #####################################################################################################################################################################################################################   \n",
    "\n",
    "            seq_orders = []\n",
    "            for coactive_group in filtered_r_clusters_df.coactive_cluster_group.unique():\n",
    "                coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                if len(filtered_r_clusters_df[coactive_group_mask]) > 1:\n",
    "                    # reorder base don first spike time order:\n",
    "                    re_ordering = np.argsort(filtered_r_clusters_df[coactive_group_mask].first_spike_time.values)\n",
    "                    seq_orders += [list(filtered_r_clusters_df[coactive_group_mask].cluster_seq_type.values[re_ordering])]\n",
    "\n",
    "            fragments =seq_orders\n",
    "\n",
    "            transitions = {}\n",
    "\n",
    "            # Iterate over each fragment\n",
    "            for fragment in fragments:\n",
    "                # Iterate over each pair of adjacent elements in the fragment\n",
    "                for i in range(len(fragment) - 1):\n",
    "                    current = fragment[i]\n",
    "                    next_element = fragment[i + 1]\n",
    "\n",
    "                    # Check if the current element is already a key in the transitions dictionary\n",
    "                    if current not in transitions:\n",
    "                        transitions[current] = {}\n",
    "\n",
    "                    # Check if the next element is already a key in the nested dictionary for the current element\n",
    "                    if next_element not in transitions[current]:\n",
    "                        transitions[current][next_element] = 1\n",
    "                    else:\n",
    "                        transitions[current][next_element] += 1\n",
    "\n",
    "            # Define the desired order of elements\n",
    "            element_order = list(extended_list+1)\n",
    "\n",
    "            element_order_reversed = list(reversed(element_order))\n",
    "\n",
    "            # Create an empty matrix with dimensions equal to the number of elements\n",
    "            matrix = np.zeros((len(element_order), len(element_order)))\n",
    "\n",
    "            # Fill the matrix with the transition counts\n",
    "\n",
    "            for i, current in enumerate(element_order):\n",
    "                for j, next_element in enumerate(element_order):\n",
    "                    if current in transitions and next_element in transitions[current]:\n",
    "                        matrix[i][j] = transitions[current][next_element]\n",
    "\n",
    "            # Normalize the matrix by column\n",
    "            matrix = matrix[::-1]\n",
    "            col_sums = matrix.sum(axis=0)\n",
    "            normalized_matrix = matrix / col_sums\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            im = ax.imshow(normalized_matrix, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "            # Add colored squares to represent the true sequence\n",
    "            logical_order = ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            for i, current in enumerate(element_order[0:len(logical_order)]):\n",
    "                for j, next_element in enumerate(element_order_reversed[0:len(logical_order)]):\n",
    "                    if current == next_element:\n",
    "                        j = j-1\n",
    "                        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='white')\n",
    "                        ax.add_patch(rect)\n",
    "\n",
    "            logical_order = ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            for i, current in enumerate(element_order_reversed[0:len(logical_order)]):\n",
    "                for j, next_element in enumerate(element_order[0:len(logical_order)]):\n",
    "                    if current == next_element:          \n",
    "                        i = i+1\n",
    "                        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='grey')\n",
    "                        ax.add_patch(rect)\n",
    "\n",
    "            # Plot the heatmap\n",
    "\n",
    "            plt.colorbar(im,label='Normalized Transition Count')\n",
    "            plt.xticks(range(len(element_order)), element_order)\n",
    "            plt.yticks(range(len(element_order)), element_order_reversed)\n",
    "            plt.xlabel('first motif')\n",
    "            plt.ylabel('second motif')\n",
    "            plt.title('Transition Heatmap - white squares = forward order, grey = reverse')\n",
    "\n",
    "            # color labels by motif/seq colour\n",
    "            for i, tick_label in enumerate(ax.axes.get_yticklabels()):\n",
    "                tick_label.set_color(colors[element_order[::-1][i]])\n",
    "                tick_label.set_fontsize(\"15\")\n",
    "            for i, tick_label in enumerate(ax.axes.get_xticklabels()):\n",
    "                tick_label.set_color(colors[element_order[i]])\n",
    "                tick_label.set_fontsize(\"15\")\n",
    "\n",
    "            # plt.show()\n",
    "            SaveFig('coaive_motif_transitions_normalised___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # PPseq warp values used\n",
    "\n",
    "            # to extract warps... for each sequence type, across last 100 iterations, I find warp values that are reported for that that type between relevant window. Add them as relative values for each seq\n",
    "            # its possible for more than one warp value to contriubute to what i define as a sequence. so for each seq I mark the relative contributions of each warp to each sequence. I then sum these for all sequences. \n",
    "            #####################################################################################################################################################################################################################  \n",
    "\n",
    "\n",
    "            # Get the index of the -1 split markers in the latent event history dataframe\n",
    "            end_markers = latent_event_history_df.loc[latent_event_history_df['seq_type'] == -1.0].index\n",
    "            # Extract the relevant portion of the latent event history dataframe\n",
    "            latent_event_history_df_split_final_100 =  latent_event_history_df[end_markers[-100]:end_markers[-1]]\n",
    "\n",
    "\n",
    "            all_warps_present = latent_event_history_df_split.seq_warp.unique()\n",
    "            relative_warps_per_seq = []\n",
    "            for index, row in filtered_r_clusters_df.iterrows():\n",
    "                event_seq_type = row.cluster_seq_type\n",
    "                first_spiketime = row.first_spike_time\n",
    "                last_spiketime = row.last_spike_time\n",
    "                latent_df_seq = latent_event_history_df_split_final_100[latent_event_history_df_split_final_100.seq_type == event_seq_type]\n",
    "\n",
    "                seq_warps = list(latent_df_seq[(latent_df_seq.timestamp >= first_spiketime-0.05) * (latent_df_seq.timestamp <= last_spiketime+0.05)].seq_warp.values)\n",
    "                counts = []\n",
    "                for item in sorted(all_warps_present):\n",
    "                    counts += [seq_warps.count(item)]\n",
    "                relative_warps_per_seq += [list(np.array(counts)/sum(counts))]\n",
    "\n",
    "\n",
    "            relative__contributions_summed = []\n",
    "            for index,item in enumerate(conactinate_nth_items(relative_warps_per_seq)):\n",
    "                relative__contributions_summed += [np.nansum(item)]\n",
    "\n",
    "            labels = np.round(np.array(sorted(all_warps_present)),1).astype(str)\n",
    "\n",
    "            nrow = 1 \n",
    "            ncol = 1\n",
    "            fig, axs = plt.subplots(nrow, ncol,figsize=(15, 8))\n",
    "\n",
    "            for ind, ax in enumerate(fig.axes):\n",
    "\n",
    "                ax.plot(relative__contributions_summed)\n",
    "\n",
    "                ax.set_xticks(range(0,len(labels)))\n",
    "                ax.set_xticklabels(labels)\n",
    "\n",
    "            plt.xlabel('warp value')\n",
    "            plt.ylabel('summed realtive occrance ')\n",
    "            plt.title('Transition Heatmap - white squares = forward order, grey = reverse')\n",
    "\n",
    "\n",
    "            np.save(chunk_path+ 'summed_relative_warp_contributions.npy',relative__contributions_summed)\n",
    "\n",
    "            SaveFig('warp_values_averaged_acorss_iterations___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # the rest of this is stuff about catagorisation (ordered,reverse,repeat,misordered) and when the reward motif occurrs. I think I probbaly dont need it but am leaving it in for posterity \n",
    "\n",
    "            #####################################################################################################################################################################################################################          \n",
    "            ############################################################ catagorisation\n",
    "\n",
    "            ### step 1, how muhc of each classification do we have. \n",
    "\n",
    "\n",
    "            ### plot proportions for chunk\n",
    "            # eventually plot proportions over time \n",
    "            # save out this data + the time it happened - this is super important for both a timeing plot and LFP analysis. \n",
    "\n",
    "            # create empty df \n",
    "            multi_cluster_df = pd.DataFrame({'cluster_seq_type':[],\n",
    "             'num_spikes':[],\n",
    "             'num_neurons':[],\n",
    "             'first_spike_time':[],\n",
    "             'event_length':[],\n",
    "             'last_spike_time':[],\n",
    "             'cluster_spike_times':[],\n",
    "             'cluster_neurons':[],\n",
    "             'spike_plotting_order':[],\n",
    "             'coactive_cluster_group':[],\n",
    "             'new_cluster_group':[],\n",
    "             'cluster_order_first_spike_defined':[],\n",
    "             'cluster_order_mean_weighted_spikes_defined':[],\n",
    "             'pairs_mean_ordering':[],\n",
    "             'catagories_mean_ordering':[],\n",
    "             'pairs_fs_ordering':[],\n",
    "             'catagories_fs_ordering':[],\n",
    "             'real_sequence_order':[]})\n",
    "            meaned_order = []\n",
    "            fs_order = []\n",
    "            event_times = []\n",
    "            multi_cluster_df\n",
    "            count = 0\n",
    "            for i,group in enumerate(filtered_r_clusters_df.coactive_cluster_group.unique()):\n",
    "                group_mask = filtered_r_clusters_df.coactive_cluster_group == group\n",
    "                current_cluster = filtered_r_clusters_df[group_mask]\n",
    "                if len(current_cluster) > 1:\n",
    "                    means = []\n",
    "                    event_types = []\n",
    "                    fs_orders = []\n",
    "                    for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                        event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                        # calculate event order based on spike time weighted mean\n",
    "                        means += [np.mean(events)]\n",
    "                        # calculate order based on first spike time:\n",
    "                        fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "                    # order by mean time:    \n",
    "                    meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "                    # order by first spike:\n",
    "                    fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "                    event_times += [fs_orders]\n",
    "\n",
    "                    current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "                    current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "                    current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "                    if count == 0:\n",
    "                        multi_cluster_df = current_cluster.copy()\n",
    "                    else:\n",
    "                        # Concatenate the DataFrames vertically (row-wise)\n",
    "                        multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                        # Reset the index if needed\n",
    "                        multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            if len( multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "\n",
    "                real_order = list(np.array(seq_order)+1)\n",
    "\n",
    "                # # mean ordering first : \n",
    "                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.bar(labels,summed_amounts)\n",
    "                ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "                SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                all_pair_outcomes_todf = []\n",
    "                all_pairs_todf = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    all_pairs = []\n",
    "                    all_pair_outcomes = []\n",
    "                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                        all_pairs += [pair_]\n",
    "                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "                multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "                # # first spike ordering second : \n",
    "                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,fs_order)\n",
    "                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.bar(labels,summed_amounts)\n",
    "                ax.set_title('catagory occurances (seqs ordered by first spike times)')\n",
    "\n",
    "                SaveFig('catagory occurances_2___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                all_pair_outcomes_todf = []\n",
    "                all_pairs_todf = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    all_pairs = []\n",
    "                    all_pair_outcomes = []\n",
    "                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                        all_pairs += [pair_]\n",
    "                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                multi_cluster_df['pairs_fs_ordering'] = all_pairs_todf\n",
    "                multi_cluster_df['catagories_fs_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "\n",
    "                multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "                multi_cluster_df.to_csv(chunk_path + 'multi_event_clusters_df.csv', index=False)\n",
    "\n",
    "\n",
    "                ########### CATAGORISATION STUFF ###########################################################################################\n",
    "\n",
    "\n",
    "                ### plot of a comparision of pair ratios during behaviour and during task. firts just plot them next to each other, then somekind of comparisoon? \n",
    "                # save this data out.\n",
    "\n",
    "                #### load in awake task data dn determine awake sequence ordering:\n",
    "\n",
    "                with open(awake_PP_path + awake_file + r'\\analysis_output\\\\' + 'spikes_seq_type_adjusted.pickle', 'rb') as handle:\n",
    "                    unmasked_spikes_df = pickle.load(handle)\n",
    "\n",
    "                with open(awake_PP_path + awake_file + r'\\analysis_output\\reordered_recolored\\\\' + 'neuron_order', 'rb') as handle:\n",
    "                    awake_neuron_order = pickle.load(handle)\n",
    "\n",
    "\n",
    "                seqs = np.unique(unmasked_spikes_df.sequence_type_adjusted)\n",
    "                seq_spikes = []\n",
    "                for seq_type_ in seqs:  \n",
    "                    seq_spikes += [unmasked_spikes_df.timestamp[np.where(unmasked_spikes_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "\n",
    "                # Define the bin size (in this case, 0.2s)\n",
    "                bin_size = 0.2\n",
    "\n",
    "                seq_spike_occurance = []\n",
    "                for spikes_ in seq_spikes:\n",
    "                    # Use the numpy.histogram function to bin the data\n",
    "                    hist, bins = np.histogram(spikes_, bins=np.arange(0, np.diff(behav_time_interval_start)[0], bin_size))\n",
    "                    seq_spike_occurance += [list(hist)]\n",
    "\n",
    "\n",
    "                #### first define when seqs occur and label neurons that contribute to them \n",
    "\n",
    "\n",
    "                seq_size_threshold= 5\n",
    "\n",
    "\n",
    "                all_seq_neurons = []  \n",
    "                df_seq_inds = []\n",
    "                total_seqs_by_type =[]\n",
    "                seq_numbers_passed = []\n",
    "                current_order = []\n",
    "                all_mid_point_times = []\n",
    "                for i in range(1,7):\n",
    "                    print(i)\n",
    "\n",
    "                    seq_spike_count = seq_spike_occurance[i]\n",
    "                    # find seq start and end, defined by whetehr there were spikes or not \n",
    "                    groups = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "                    ### plot to check that I am accounting ofr sequences properly\n",
    "\n",
    "                    #mask\n",
    "                    # spikemask\n",
    "                    timeframe = [0,800]\n",
    "                    mask = (unmasked_spikes_df.timestamp>timeframe[0])*(unmasked_spikes_df.timestamp<timeframe[-1])\n",
    "                    background_remove_mask = unmasked_spikes_df[mask].sequence_type_adjusted >= 0\n",
    "                    c_ = np.array(colors)[unmasked_spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "\n",
    "                    fig,[ax1,ax2] = plt.subplots(2, 1,figsize=(10, 5))\n",
    "                    ax1.scatter(unmasked_spikes_df[mask][background_remove_mask].timestamp, awake_neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "                    ax2.plot(seq_spike_count, color = colors[i])\n",
    "                    for item in groups:\n",
    "                        ax2.plot(item,[-5,-5], color = 'red')\n",
    "\n",
    "\n",
    "                    ax1.set_xlim([0,100])\n",
    "                    ax2.set_xlim([0,(100/bin_size)])\n",
    "\n",
    "                    seq_neurons = []\n",
    "                    df_index = []\n",
    "                    mid_point_time = []\n",
    "\n",
    "                    counter = 0\n",
    "                    for group in groups:\n",
    "\n",
    "                        # spikemask\n",
    "                        timeframe = [(group[0] * bin_size)-0.5,(group[-1] * bin_size)+0.5]\n",
    "                        mid_point_time += [timeframe[0] + (np.diff(timeframe)[0]/2)]\n",
    "                        mask = (unmasked_spikes_df.timestamp>timeframe[0])*(unmasked_spikes_df.timestamp<timeframe[-1])\n",
    "                        seq_mask = unmasked_spikes_df[mask].sequence_type_adjusted == i\n",
    "\n",
    "                        if len(unmasked_spikes_df[mask][seq_mask]) > seq_size_threshold:\n",
    "                            counter +=1\n",
    "                            seq_neurons.append(list(unmasked_spikes_df[mask][seq_mask].neuron))\n",
    "                            df_index += [list(unmasked_spikes_df[mask][seq_mask].index)]\n",
    "\n",
    "                    total_seqs_by_type += [counter]\n",
    "                    all_seq_neurons.append(seq_neurons)\n",
    "                    df_seq_inds.append(df_index)\n",
    "                    all_mid_point_times += [mid_point_time]\n",
    "\n",
    "                    current_order +=[i]\n",
    "\n",
    "                flat_seqs = []\n",
    "                for index,item in enumerate(all_mid_point_times):\n",
    "                    flat_seqs += [current_order[index]]*len(item)\n",
    "                flat_seq_times = [item for sublist in all_mid_point_times for item in sublist]\n",
    "                awake_seq_order = list(np.array(flat_seqs)[np.argsort(flat_seq_times)])\n",
    "\n",
    "\n",
    "\n",
    "                #### plot event transitions freqs for awake and sleep: \n",
    "\n",
    "\n",
    "                awake_event_transitions = count_event_transitions(awake_seq_order)\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'awake_event_transitions.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(awake_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # Print the frequency of each event transition\n",
    "                # for transition, count in awake_event_transitions.items():\n",
    "                #     print(f\"Transition {transition}: {count:.2f}% occurrences\")\n",
    "\n",
    "\n",
    "                fig,[ax,ax2] = plt.subplots(1, 2,figsize=(20, 5))\n",
    "                plot_event_transitions(awake_event_transitions,ax,'')\n",
    "                plot_event_transitions(awake_event_transitions,ax2,'')\n",
    "\n",
    "                ### do the same for sleep:\n",
    "\n",
    "                mean_ordering_pairs = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    current_group = multi_cluster_df[multi_cluster_df.new_cluster_group == group]\n",
    "                    mean_ordering_pairs += current_group.pairs_mean_ordering.values[0]\n",
    "\n",
    "                sleep_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for pair in mean_ordering_pairs:\n",
    "                    tup_pair = pair[0],pair[-1]\n",
    "                    if tup_pair in sleep_event_transitions:\n",
    "                        sleep_event_transitions[tup_pair] += 1\n",
    "                    else:\n",
    "                        sleep_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                mean_ordered_sleep_event_transitions = normalize_counts_to_percentages(sleep_event_transitions)\n",
    "                plot_event_transitions(mean_ordered_sleep_event_transitions,ax,'- mean spike times ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'sleep_event_transitions_mean_spike_time_ordered.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(mean_ordered_sleep_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "                fs_ordering_pairs = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    current_group = multi_cluster_df[multi_cluster_df.new_cluster_group == group]\n",
    "                    fs_ordering_pairs += current_group.pairs_fs_ordering.values[0]\n",
    "\n",
    "                sleep_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for pair in fs_ordering_pairs:\n",
    "                    tup_pair = pair[0],pair[-1]\n",
    "                    if tup_pair in sleep_event_transitions:\n",
    "                        sleep_event_transitions[tup_pair] += 1\n",
    "                    else:\n",
    "                        sleep_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                fs_ordered_sleep_event_transitions = normalize_counts_to_percentages(sleep_event_transitions)\n",
    "                plot_event_transitions(fs_ordered_sleep_event_transitions,ax2, '- first spike ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'sleep_event_transitions_first_spike_time_ordered.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(fs_ordered_sleep_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                ax.text(1,23,'sleep transitions',color = 'orange',size = 10)\n",
    "                ax.text(1,22,'Awake transitions', color = 'blue',size = 10)\n",
    "                ax.text(0,-5,'sequence order =' + ','.join(list((np.array(seq_order)+1).astype(str))) + '   |   num dominant = ' +  str(num_dominant_seqs), color = 'blue',size = 10)\n",
    "\n",
    "                SaveFig('event_transitions_awake_sleep___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            #             for key in fs_ordered_sleep_event_transitions:\n",
    "            #                 if not key in awake_event_transitions:\n",
    "            #                     awake_event_transitions[key] = 0\n",
    "            #             for key in mean_ordered_sleep_event_transitions:\n",
    "            #                 if not key in awake_event_transitions:\n",
    "            #                     awake_event_transitions[key] = 0\n",
    "\n",
    "                #  plot correlation all\n",
    "                fig,[ax,ax2]= plt.subplots(1, 2,figsize=(10, 5))\n",
    "                sns.regplot(y=list(fs_ordered_sleep_event_transitions.values()), x=list(awake_event_transitions.values()), ax = ax)\n",
    "                sns.regplot(y=list(mean_ordered_sleep_event_transitions.values()), x=list(awake_event_transitions.values()), ax = ax2)\n",
    "                ax.set_title('transition event freqs: awake vs sleep (first spike ordered)', size =7)\n",
    "                ax2.set_title('transition event freqs: awake vs sleep (mean spike time ordered)', size =7)\n",
    "                ax.set_xlabel(' awake transition event freq (%)')\n",
    "                ax.set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                SaveFig('transition_events_awake_sleep_frequencies___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                ### plot correlation for each catagory ####################################################################################\n",
    "\n",
    "                #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "                if not len(real_order) == num_dominant_seqs:\n",
    "                    dominant = real_order[0:num_dominant_seqs]\n",
    "                    other_ = real_order[num_dominant_seqs::]\n",
    "                else:\n",
    "                    dominant = real_order\n",
    "                    other_ = []\n",
    "\n",
    "                outcome= []\n",
    "                pairs = []\n",
    "                for pair in list(awake_event_transitions.keys()):\n",
    "                    pairs += [list(pair)]\n",
    "                    outcome += [logic_machine_for_pair_catagorisation(list(pair),dominant,other_)]\n",
    "\n",
    "                awake_event_freqs_df = pd.DataFrame({'transition': pairs,'catagory':outcome,'awake_event_transitions':list(awake_event_transitions.values()),'mean_spike_ordered_sleep_frequencies':list(mean_ordered_sleep_event_transitions.values()),'fs_ordered_sleep_frequencies':list(fs_ordered_sleep_event_transitions.values())})\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Assuming you have a dataframe named 'df' with columns 'categories', 'datax', and 'datay'\n",
    "\n",
    "                # Group the dataframe by 'categories'\n",
    "                grouped_df = awake_event_freqs_df.groupby('catagory')\n",
    "\n",
    "                # Get the number of unique categories\n",
    "                num_categories = len(grouped_df)\n",
    "\n",
    "                # Create subplots\n",
    "                fig, axes = plt.subplots(num_categories, 1, figsize=(5, 5*num_categories))\n",
    "\n",
    "                # Iterate over each category and subplot\n",
    "                for i, (category, group) in enumerate(grouped_df):\n",
    "                    awake_event_freqs = group.awake_event_transitions.values\n",
    "                    mo_sleep_freqs = group.mean_spike_ordered_sleep_frequencies.values\n",
    "                    fs_sleep_freqs = group.fs_ordered_sleep_frequencies.values\n",
    "\n",
    "                    sns.regplot(y=mo_sleep_freqs, x=awake_event_freqs, ax = axes[i])\n",
    "                    sns.regplot(y=fs_sleep_freqs, x=awake_event_freqs, ax = axes[i])\n",
    "                    axes[i].set_title(category + ' transition event freqs awake vs sleep  ' + '---->' + '  blue = mean ordered | orange = first spike ordered', size = 6)\n",
    "\n",
    "                    axes[i].set_xlabel('awake transition event freq (%)')\n",
    "                    axes[i].set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                # Adjust spacing between subplots\n",
    "                plt.tight_layout()\n",
    "\n",
    "                awake_event_freqs_df.to_csv(chunk_path + 'transit_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                SaveFig('transition_events_awake_sleep_frequencies_CATAGORIZED___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                #################################### comparision of reward related transitoions during awake \n",
    "\n",
    "                ### load in reward times \n",
    "\n",
    "                behav_data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "                os.listdir(behav_data_path)\n",
    "                mouse_session_recording = mir\n",
    "\n",
    "                ## set dat_path:\n",
    "                for file_ in os.listdir(behav_data_path):\n",
    "                    if mouse_session_recording.split('_')[0] in file_:\n",
    "                        if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                            dat_path = os.path.join(behav_data_path,file_)\n",
    "                for recording in os.listdir(os.path.join(behav_data_path,dat_path)):\n",
    "                    if recording.split('_')[0][-1] == mouse_session_recording.split('_')[-1]:\n",
    "                        dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "                for file in os.listdir(dat_path + r'\\behav_sync\\\\'):\n",
    "                    if 'task' in file:\n",
    "                        behav_sync = pd.read_csv(dat_path + r'\\behav_sync\\\\' + file + '\\Behav_Ephys_Camera_Sync.csv')\n",
    "\n",
    "\n",
    "                reward_times_bpod = behav_sync.Reward_Times.values\n",
    "                mask = np.isnan(reward_times_bpod)\n",
    "                # Use the mask to filter out NaN values\n",
    "                reward_times = behav_sync.PokeIN_EphysTime.values[~mask]\n",
    "\n",
    "                # account for time window:\n",
    "                params_file = awake_PP_path + awake_file + r'\\trainingData\\\\' + 'params_' + mir + '.json'\n",
    "                with open(params_file, 'r') as file:\n",
    "                    params = json.load(file)\n",
    "                time_span = params['time_span'][0]\n",
    "                time_span_mask = (reward_times > time_span[0])*(reward_times < time_span[-1])\n",
    "                reward_times = reward_times[time_span_mask]\n",
    "                # convert to relative time\n",
    "                reward_times = reward_times - params['time_span'][0][0]\n",
    "\n",
    "                # detemrine seqs that occur within 3s of reward\n",
    "                sorted_seq_times = np.array(sorted(flat_seq_times))\n",
    "                awake_seq_order\n",
    "\n",
    "                reward_related_seqs = []\n",
    "                for r_time in reward_times:\n",
    "                    r_time_window_start = r_time - 2 \n",
    "                    window_mask = (sorted_seq_times >= r_time_window_start) * (sorted_seq_times <= r_time)\n",
    "                    reward_related_seqs += [list(np.array(awake_seq_order)[window_mask])]\n",
    "\n",
    "                reward_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for seq in reward_related_seqs:\n",
    "                    for index,element in enumerate(seq[0:-1]):\n",
    "                        pair = [element,seq[index+1]]\n",
    "                        tup_pair = pair[0],pair[-1]\n",
    "                        if tup_pair in sleep_event_transitions:\n",
    "                            reward_event_transitions[tup_pair] += 1\n",
    "                        else:\n",
    "                            reward_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                reward_event_transitions = normalize_counts_to_percentages(reward_event_transitions)\n",
    "\n",
    "                fig,[ax,ax2] = plt.subplots(1, 2,figsize=(20, 5))\n",
    "\n",
    "                plot_event_transitions(reward_event_transitions,ax, '- mean spike ordered')\n",
    "                plot_event_transitions(mean_ordered_sleep_event_transitions,ax, '- first spike ordered')\n",
    "\n",
    "                ax.text(1,23,'sleep transitions',color = 'orange',size = 10)\n",
    "                ax.text(1,22,'reward related awake transitions', color = 'blue',size = 10)\n",
    "                ax.text(0,-5,'sequence order =' + ','.join(list((np.array(seq_order)+1).astype(str))) + '   |   num dominant = ' +  str(num_dominant_seqs), color = 'blue',size = 10)\n",
    "\n",
    "                plot_event_transitions(reward_event_transitions,ax2, '- first spike ordered')\n",
    "                plot_event_transitions(fs_ordered_sleep_event_transitions,ax2, '- first spike ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'reward_releated_awake_event_transitions.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(reward_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                SaveFig('event_transitions_reward_related_awake_sleep___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                #  plot correlation all\n",
    "\n",
    "                fig,[ax,ax2]= plt.subplots(1, 2,figsize=(10, 5))\n",
    "                sns.regplot(y=list(fs_ordered_sleep_event_transitions.values()), x=list(reward_event_transitions.values()), ax = ax)\n",
    "                sns.regplot(y=list(mean_ordered_sleep_event_transitions.values()), x=list(reward_event_transitions.values()), ax = ax2)\n",
    "                ax.set_title('transition event freqs: awake vs sleep (first spike ordered)', size =7)\n",
    "                ax2.set_title('transition event freqs: awake vs sleep (mean spike time ordered)', size =7)\n",
    "                ax.set_xlabel(' awake transition event freq (%)')\n",
    "                ax.set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                SaveFig('transition_events_REWARDEDawake_sleep_frequencies____chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                ### plot correlation for each catagory ####################################################################################\n",
    "\n",
    "                #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "                if not len(real_order) == num_dominant_seqs:\n",
    "                    dominant = real_order[0:num_dominant_seqs]\n",
    "                    other_ = real_order[num_dominant_seqs::]\n",
    "                else:\n",
    "                    dominant = real_order\n",
    "                    other_ = []\n",
    "\n",
    "                outcome= []\n",
    "                pairs = []\n",
    "                for pair in list(reward_event_transitions.keys()):\n",
    "                    pairs += [list(pair)]\n",
    "                    outcome += [logic_machine_for_pair_catagorisation(list(pair),dominant,other_)]\n",
    "\n",
    "                reward_event_freqs_df = pd.DataFrame({'transition': pairs,'catagory':outcome,'reward_event_transitions':list(reward_event_transitions.values()),'mean_spike_ordered_sleep_frequencies':list(mean_ordered_sleep_event_transitions.values()),'fs_ordered_sleep_frequencies':list(fs_ordered_sleep_event_transitions.values())})\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Assuming you have a dataframe named 'df' with columns 'categories', 'datax', and 'datay'\n",
    "\n",
    "                # Group the dataframe by 'categories'\n",
    "                grouped_df = reward_event_freqs_df.groupby('catagory')\n",
    "\n",
    "                # Get the number of unique categories\n",
    "                num_categories = len(grouped_df)\n",
    "\n",
    "                # Create subplots\n",
    "                fig, axes = plt.subplots(num_categories, 1, figsize=(5, 5*num_categories))\n",
    "\n",
    "                # Iterate over each category and subplot\n",
    "                for i, (category, group) in enumerate(grouped_df):\n",
    "                    r_event_freqs = group.reward_event_transitions.values\n",
    "                    mo_sleep_freqs = group.mean_spike_ordered_sleep_frequencies.values\n",
    "                    fs_sleep_freqs = group.fs_ordered_sleep_frequencies.values\n",
    "\n",
    "                    sns.regplot(y=mo_sleep_freqs, x=r_event_freqs, ax = axes[i])\n",
    "                    sns.regplot(y=fs_sleep_freqs, x=r_event_freqs, ax = axes[i])\n",
    "                    axes[i].set_title(category + ' transition event freqs awake vs sleep  ' + '---->' + '  blue = mean ordered | orange = first spike ordered', size = 6)\n",
    "\n",
    "                    axes[i].set_xlabel('reward related awake transition event freq (%)')\n",
    "                    axes[i].set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                reward_event_freqs_df.to_csv(chunk_path + 'Rewarded_transit_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                # Adjust spacing between subplots\n",
    "                plt.tight_layout()\n",
    "\n",
    "                SaveFig('transition_events_REWARDEDawake_sleep_frequencies_CATAGORIZED___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                ####################################################################################################################################\n",
    "\n",
    "                ## plot and save out proportions for each sequene in awake and sleep and reward awake and sleep\n",
    "\n",
    "\n",
    "                reward_awake_counts = []\n",
    "                all_awake_counts = []\n",
    "                sleep_counts = []\n",
    "                for i in range(1,7):\n",
    "                    flat = [item for sublist in reward_related_seqs for item in sublist]\n",
    "                    reward_awake_counts += [flat.count(i)]\n",
    "                    all_awake_counts += [awake_seq_order.count(i)]\n",
    "                    sleep_counts += [list(filtered_r_clusters_df.cluster_seq_type.values).count(i)]\n",
    "\n",
    "                reward_awake_props = np.array(reward_awake_counts)/sum(reward_awake_counts)*100\n",
    "                awake_props = np.array(all_awake_counts)/sum(all_awake_counts)*100\n",
    "                sleep_props = np.array(sleep_counts)/sum(sleep_counts)*100\n",
    "\n",
    "                reward_diff = reward_awake_props - awake_props\n",
    "\n",
    "                import scipy\n",
    "\n",
    "                fig,ax= plt.subplots(1, 1,figsize=(5, 5))\n",
    "                sns.regplot(y=sleep_props, x=awake_props, ax = ax)\n",
    "                ax.set_xlabel(' awake single event freq (%)')\n",
    "                ax.set_ylabel('sleep single event freq (%)')\n",
    "                ax.axhline(0,0,ls ='--')\n",
    "\n",
    "                sns.regplot(y=sleep_props, x=reward_awake_props, ax = ax)\n",
    "\n",
    "                ax.text(0,60,'compared to all all awake events', color = 'blue',size = 10)\n",
    "                ax.text(0,55,'compared to reward related awake events', color = 'orange',size = 10)\n",
    "\n",
    "                event_freqs_df = pd.DataFrame({'seq': range(1,7),'awake_frequencies':awake_props,'reward_related_awake_frequencies':reward_awake_props,'sleep_frequencies':sleep_props})\n",
    "\n",
    "                event_freqs_df.to_csv(chunk_path + 'single_event_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                SaveFig('SINGLE_EVENT_awake_awakereward_sleep_freqs___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "                \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7307e5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\ppseq_data\\\\output_data\\\\striatum\\\\New_Post_sleep\\\\\\\\270_1_6_run_1906023_2327\\\\analysis_output\\\\chunk3_16000to17000\\\\'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fb04540",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_df = pd.read_csv(dat_path + r\"\\\\behav_sync\\3_post_sleep\\Postsleep_Ephys_Camera_sync.csv\")\n",
    "start = sync_df.Camera_time_Ephys_Aligned.values[0]\n",
    "end = sync_df.Camera_time_Ephys_Aligned.values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccfdb2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8099.190641203198"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrem_start_ends = np.load(dat_path + '/ephys/LFP/sleep_state_score/nrem_start_ends.npy')\n",
    "rem_start_ends = np.load(dat_path + '/ephys/LFP/sleep_state_score/rem_start_ends.npy')\n",
    "\n",
    "## plot out all replays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9abec573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\EJT136_implant1\\\\recording3_11-11-2021\\\\behav_sync\\x03_post_sleep\\\\Postsleep_Ephys_Camera_sync.csv'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d24bf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\EJT136_implant1\\\\recording3_11-11-2021'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04a9a3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time Stamps</th>\n",
       "      <th>Trigger State</th>\n",
       "      <th>DataPath</th>\n",
       "      <th>Camera_time_raw</th>\n",
       "      <th>Camera_trigger_times</th>\n",
       "      <th>Ephys_trigger_times</th>\n",
       "      <th>Camera_time_Ephys_Aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8099.190641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8099.207266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8099.223891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8099.240641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.066625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>0.066625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8099.257266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311293</th>\n",
       "      <td>311293</td>\n",
       "      <td>5185.789500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>5185.789500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13284.695245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311294</th>\n",
       "      <td>311294</td>\n",
       "      <td>5185.806125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>5185.806125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13284.711870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311295</th>\n",
       "      <td>311295</td>\n",
       "      <td>5185.822750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>5185.822750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13284.728495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311296</th>\n",
       "      <td>311296</td>\n",
       "      <td>5185.839500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>5185.839500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13284.745245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311297</th>\n",
       "      <td>311297</td>\n",
       "      <td>5185.856125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z:\\projects\\sequence_squad\\data\\video_data\\raw...</td>\n",
       "      <td>5185.856125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13284.761870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311298 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Time Stamps  Trigger State  \\\n",
       "0                0     0.000000            0.0   \n",
       "1                1     0.016625            0.0   \n",
       "2                2     0.033250            0.0   \n",
       "3                3     0.050000            0.0   \n",
       "4                4     0.066625            0.0   \n",
       "...            ...          ...            ...   \n",
       "311293      311293  5185.789500            0.0   \n",
       "311294      311294  5185.806125            0.0   \n",
       "311295      311295  5185.822750            0.0   \n",
       "311296      311296  5185.839500            0.0   \n",
       "311297      311297  5185.856125            0.0   \n",
       "\n",
       "                                                 DataPath  Camera_time_raw  \\\n",
       "0       Z:\\projects\\sequence_squad\\data\\video_data\\raw...         0.000000   \n",
       "1       Z:\\projects\\sequence_squad\\data\\video_data\\raw...         0.016625   \n",
       "2       Z:\\projects\\sequence_squad\\data\\video_data\\raw...         0.033250   \n",
       "3       Z:\\projects\\sequence_squad\\data\\video_data\\raw...         0.050000   \n",
       "4       Z:\\projects\\sequence_squad\\data\\video_data\\raw...         0.066625   \n",
       "...                                                   ...              ...   \n",
       "311293  Z:\\projects\\sequence_squad\\data\\video_data\\raw...      5185.789500   \n",
       "311294  Z:\\projects\\sequence_squad\\data\\video_data\\raw...      5185.806125   \n",
       "311295  Z:\\projects\\sequence_squad\\data\\video_data\\raw...      5185.822750   \n",
       "311296  Z:\\projects\\sequence_squad\\data\\video_data\\raw...      5185.839500   \n",
       "311297  Z:\\projects\\sequence_squad\\data\\video_data\\raw...      5185.856125   \n",
       "\n",
       "        Camera_trigger_times  Ephys_trigger_times  Camera_time_Ephys_Aligned  \n",
       "0                        NaN                  NaN                8099.190641  \n",
       "1                        NaN                  NaN                8099.207266  \n",
       "2                        NaN                  NaN                8099.223891  \n",
       "3                        NaN                  NaN                8099.240641  \n",
       "4                        NaN                  NaN                8099.257266  \n",
       "...                      ...                  ...                        ...  \n",
       "311293                   NaN                  NaN               13284.695245  \n",
       "311294                   NaN                  NaN               13284.711870  \n",
       "311295                   NaN                  NaN               13284.728495  \n",
       "311296                   NaN                  NaN               13284.745245  \n",
       "311297                   NaN                  NaN               13284.761870  \n",
       "\n",
       "[311298 rows x 8 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08b7cdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x208e99048e0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15d45bf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 44 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m timeframe \u001b[38;5;241m=\u001b[39m [interval_start,\u001b[43minterval_end_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m total_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiff(timeframe)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 44 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "timeframe = [interval_start,interval_end_points[index_]-1]\n",
    "total_time = np.diff(timeframe)[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b615af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
