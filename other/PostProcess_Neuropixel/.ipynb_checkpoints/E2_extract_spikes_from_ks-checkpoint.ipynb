{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6d8c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Make notebook wider:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d41bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Fs = 30000.0\n",
    "\n",
    "def create_spike_time_vectors(spike_times,clusters):\n",
    "    spiketimevectors = []\n",
    "    for i in np.unique(clusters):\n",
    "        spiketimevectors = spiketimevectors + [spike_times[np.where(clusters==i)[0]]]\n",
    "    return spiketimevectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13bb45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "\n",
    "\n",
    "animal = 'EJT270_implant1'\n",
    "\n",
    "type_ = 'kilosort3'\n",
    "\n",
    "\n",
    "# path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\' + animal + '\\\\'\n",
    "\n",
    "path = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\" + animal + '\\\\'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "implant = 'track_0.csv'\n",
    "neurpixel_track_path = r'Z:\\projects\\sequence_squad\\data\\histology\\Neuropixel_tracks\\EJT270_NP\\brainreg\\manual_segmentation\\standard_space\\tracks\\\\'\n",
    "\n",
    "implant_df = pd.read_csv(neurpixel_track_path + implant)\n",
    "\n",
    "\n",
    "\n",
    "### should I call it motor cortex?? decide!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d786f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### possible fuck up - not used the rihgt number of points when tracing the probe track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "982f647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong number of points used!\n",
      "fix this by altering script below:\n"
     ]
    }
   ],
   "source": [
    "if not len(implant_df) == 4000:\n",
    "    print('wrong number of points used!')\n",
    "    print('fix this by altering script below:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21cd9026",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording1_05-10-2023\n",
      "Total good clusters = 192\n",
      "Total mua clusters = 235\n",
      "data saved\n",
      "recording2_05-12-2023\n",
      "Total good clusters = 130\n",
      "Total mua clusters = 202\n",
      "data saved\n",
      "recording3_05-14-2023\n",
      "Total good clusters = 193\n",
      "Total mua clusters = 226\n",
      "data saved\n",
      "recording4_05-16-2023\n",
      "Total good clusters = 168\n",
      "Total mua clusters = 374\n",
      "data saved\n",
      "recording5_05-17-2023\n",
      "Total good clusters = 128\n",
      "Total mua clusters = 282\n",
      "data saved\n",
      "recording6_05-19-2023\n",
      "Total good clusters = 118\n",
      "Total mua clusters = 187\n",
      "data saved\n",
      "recording7_05-20-2023\n",
      "Total good clusters = 120\n",
      "Total mua clusters = 223\n",
      "data saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for recording in os.listdir(path):\n",
    "    if not 'Store' in recording: # ignore ds store thing\n",
    "        print(recording)\n",
    "        current_path = os.path.join(path,recording,'ephys',type_) + '\\\\'\n",
    "        out_path = os.path.join(path,recording,'ephys')\n",
    "\n",
    "        # load in spike data and curation file:\n",
    "        spike_times = np.load(current_path + 'spike_times.npy')\n",
    "        spike_times = (np.squeeze(spike_times))/Fs\n",
    "        clusters = np.squeeze(np.load(current_path + 'spike_clusters.npy'))\n",
    "\n",
    "        #### Make spike time vectors: \n",
    "        ## cluster kslabel seems to be the same thing, I think cluster group is what get chaged in teh curation so use this. \n",
    "#         ks_label_df = pd.read_csv(current_path + 'cluster_group.tsv', sep='\\t')\n",
    "        ks_label_df = pd.read_csv(current_path + 'cluster_KSLabel.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "        # Sort spike data into vectors that can be rasters. ie. single clusters and all spike times in ephys time \n",
    "        spiketimevectors = create_spike_time_vectors(spike_times,clusters)\n",
    "\n",
    "        # pull out curation scores for each cluster\n",
    "#         ks_label = np.array(ks_label_df.loc[:,'group'])\n",
    "        ks_label = np.array(ks_label_df.loc[:,'KSLabel'])\n",
    "\n",
    "        ks_label_cluster = np.array(ks_label_df.loc[:,'cluster_id'])\n",
    "\n",
    "        # define good and mua clusters\n",
    "        mua_clusters = ks_label_cluster[ks_label == 'mua']\n",
    "        good_clusters = ks_label_cluster[ks_label == 'good']\n",
    "\n",
    "        #### This code has become redundant because of the code below, this is the old way I eextracted spike vectors, now im using the new way as it also gives me depths \n",
    "\n",
    "        # ignored_clusters= 0\n",
    "\n",
    "        # mua_spikevectors= []\n",
    "        # good_spikevectors = [] \n",
    "\n",
    "        # mua_cluster_ids = []\n",
    "        # good_cluster_ids = []\n",
    "\n",
    "        # for index,cluster in enumerate(np.unique(clusters)):\n",
    "        #     if cluster in mua_clusters:\n",
    "        #         mua_cluster_ids = mua_cluster_ids + [cluster]\n",
    "        #         mua_spikevectors = mua_spikevectors + [list(spiketimevectors[index])]\n",
    "\n",
    "        #     if cluster in good_clusters:\n",
    "        #         good_cluster_ids = good_cluster_ids + [cluster]\n",
    "        #         good_spikevectors = good_spikevectors + [list(spiketimevectors[index])]\n",
    "\n",
    "        #     else:\n",
    "        #         ignored_clusters = ignored_clusters + 1\n",
    "\n",
    "        # print('Total good clusters = ' + str(len(good_spikevectors)))\n",
    "        # print('Total mua clusters = ' + str(len(mua_spikevectors)))\n",
    "\n",
    "        #### this code is taken from Spykes, it essentially recacluates what I ahve done above but also finds the depth so i use the outp use this instead\n",
    "        ## essentially this code orders spike sby cluster, then finds which tamplate the spikes match. then it finds the depth of that template and averages all the spikes to find the depth. \n",
    "        # Smaller depth number = closer to the tip. \n",
    "\n",
    "        spike_templates = np.squeeze(np.load(current_path + 'spike_templates.npy'))\n",
    "        temps = np.load(current_path + 'templates.npy')\n",
    "        winv = np.load(current_path + 'whitening_mat_inv.npy')\n",
    "        y_coords = np.squeeze(np.load(current_path + 'channel_positions.npy'))[:,1]\n",
    "\n",
    "        ### first for good clusters \n",
    "\n",
    "        # find good clusters and only use those spikes\n",
    "        good_indices = (np.in1d(clusters, good_clusters))\n",
    "\n",
    "        real_spikes = spike_times[good_indices]\n",
    "        real_clusters = clusters[good_indices]\n",
    "        real_spike_templates = spike_templates[good_indices]\n",
    "\n",
    "        # find how many spikes per cluster and then order spikes by which cluster they are in\n",
    "        counts_per_cluster = np.bincount(real_clusters)\n",
    "\n",
    "        sort_idx = np.argsort(real_clusters)\n",
    "        sorted_clusters = real_clusters[sort_idx]\n",
    "        sorted_spikes = real_spikes[sort_idx]\n",
    "        sorted_spike_templates = real_spike_templates[sort_idx]\n",
    "\n",
    "        # find depth for each spike\n",
    "        # this is translated from Cortex Lab's MATLAB code\n",
    "        # for more details, check out the original code here:\n",
    "        # https://github.com/cortex-lab/spikes/blob/master/analysis/templatePositionsAmplitudes.m\n",
    "\n",
    "        temps_unw = np.zeros(temps.shape)\n",
    "        for t in range(temps.shape[0]):\n",
    "            temps_unw[t, :, :] = np.dot(temps[t, :, :], winv)\n",
    "\n",
    "        temp_chan_amps = np.ptp(temps_unw, axis=1)\n",
    "        temps_amps = np.max(temp_chan_amps, axis=1)\n",
    "        thresh_vals = temps_amps * 0.3\n",
    "\n",
    "        thresh_vals = [thresh_vals for i in range(temp_chan_amps.shape[1])]\n",
    "        thresh_vals = np.stack(thresh_vals, axis=1)\n",
    "\n",
    "        temp_chan_amps[temp_chan_amps < thresh_vals] = 0\n",
    "\n",
    "        y_coords = np.reshape(y_coords, (y_coords.shape[0], 1))\n",
    "        temp_depths = np.sum(\n",
    "            np.dot(temp_chan_amps, y_coords), axis=1) / (np.sum(temp_chan_amps,\n",
    "                                                         axis=1))\n",
    "\n",
    "        sorted_spike_depths = temp_depths[sorted_spike_templates]\n",
    "\n",
    "        # create neurons and find region\n",
    "        out_clusters= []\n",
    "        spike_vectors = []\n",
    "        cluster_depths = []\n",
    "\n",
    "        accumulator = 0\n",
    "\n",
    "        for idx, count in enumerate(counts_per_cluster):\n",
    "\n",
    "            if count > 0:\n",
    "\n",
    "                new_spike_times = list(np.sort(sorted_spikes[accumulator:accumulator + count]))\n",
    "                cluster_depth = np.mean(sorted_spike_depths[accumulator:accumulator + count])\n",
    "\n",
    "\n",
    "                spike_vectors = spike_vectors + [new_spike_times]\n",
    "                cluster_depths = cluster_depths + [cluster_depth]\n",
    "                out_clusters = out_clusters + [idx]\n",
    "\n",
    "                accumulator += count\n",
    "\n",
    "        # make dataframe:\n",
    "        good_df = pd.DataFrame(\n",
    "            {'cluster_id' : out_clusters,\n",
    "            'cluster_depth' : cluster_depths,\n",
    "             'Spike_times':spike_vectors})\n",
    "\n",
    "\n",
    "        ### add in region info based on depth:\n",
    "        try:\n",
    "            callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "        except:\n",
    "            callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "        \n",
    "        ## fix for error I made when I didnt give the traced tracks the right number of points\n",
    "        callosum_middle_index = (callosum_middle_index*4)\n",
    "\n",
    "        cortex_index = good_df.cluster_depth.values > callosum_middle_index\n",
    "        striatum_index = good_df.cluster_depth.values < callosum_middle_index\n",
    "\n",
    "        positions_ = ['striatum'] * len(good_df.cluster_id)\n",
    "        for index, item in enumerate(positions_):\n",
    "            if cortex_index[index]:\n",
    "                positions_[index] = 'm_cortex'\n",
    "\n",
    "        good_df['Region'] = positions_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('Total good clusters = ' + str(len(spike_vectors)))\n",
    "\n",
    "        ### then for mua clusters \n",
    "\n",
    "\n",
    "        # find good clusters and only use those spikes\n",
    "        good_indices = (np.in1d(clusters, mua_clusters))\n",
    "\n",
    "        real_spikes = spike_times[good_indices]\n",
    "        real_clusters = clusters[good_indices]\n",
    "        real_spike_templates = spike_templates[good_indices]\n",
    "\n",
    "        # find how many spikes per cluster and then order spikes by which cluster they are in\n",
    "        counts_per_cluster = np.bincount(real_clusters)\n",
    "\n",
    "        sort_idx = np.argsort(real_clusters)\n",
    "        sorted_clusters = real_clusters[sort_idx]\n",
    "        sorted_spikes = real_spikes[sort_idx]\n",
    "        sorted_spike_templates = real_spike_templates[sort_idx]\n",
    "\n",
    "        # find depth for each spike\n",
    "        # this is translated from Cortex Lab's MATLAB code\n",
    "        # for more details, check out the original code here:\n",
    "        # https://github.com/cortex-lab/spikes/blob/master/analysis/templatePositionsAmplitudes.m\n",
    "\n",
    "        temps_unw = np.zeros(temps.shape)\n",
    "        for t in range(temps.shape[0]):\n",
    "            temps_unw[t, :, :] = np.dot(temps[t, :, :], winv)\n",
    "\n",
    "        temp_chan_amps = np.ptp(temps_unw, axis=1)\n",
    "        temps_amps = np.max(temp_chan_amps, axis=1)\n",
    "        thresh_vals = temps_amps * 0.3\n",
    "\n",
    "        thresh_vals = [thresh_vals for i in range(temp_chan_amps.shape[1])]\n",
    "        thresh_vals = np.stack(thresh_vals, axis=1)\n",
    "\n",
    "        temp_chan_amps[temp_chan_amps < thresh_vals] = 0\n",
    "\n",
    "        y_coords = np.reshape(y_coords, (y_coords.shape[0], 1))\n",
    "        temp_depths = np.sum(\n",
    "            np.dot(temp_chan_amps, y_coords), axis=1) / (np.sum(temp_chan_amps,\n",
    "                                                         axis=1))\n",
    "\n",
    "        sorted_spike_depths = temp_depths[sorted_spike_templates]\n",
    "\n",
    "        # create neurons and find region\n",
    "        out_clusters= []\n",
    "        spike_vectors = []\n",
    "        cluster_depths = []\n",
    "\n",
    "        accumulator = 0\n",
    "\n",
    "        for idx, count in enumerate(counts_per_cluster):\n",
    "\n",
    "            if count > 0:\n",
    "\n",
    "                new_spike_times = list(np.sort(sorted_spikes[accumulator:accumulator + count]))\n",
    "                cluster_depth = np.mean(sorted_spike_depths[accumulator:accumulator + count])\n",
    "\n",
    "\n",
    "                spike_vectors = spike_vectors + [new_spike_times]\n",
    "                cluster_depths = cluster_depths + [cluster_depth]\n",
    "                out_clusters = out_clusters + [idx]\n",
    "\n",
    "                accumulator += count\n",
    "\n",
    "        # make dataframe:\n",
    "        mua_df = pd.DataFrame(\n",
    "            {'cluster_id' : out_clusters,\n",
    "            'cluster_depth' : cluster_depths,\n",
    "             'Spike_times':spike_vectors})\n",
    "\n",
    "\n",
    "        ### add in region info based on depth:\n",
    "\n",
    "        try:\n",
    "            callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "        except:\n",
    "            callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "\n",
    "        cortex_index = mua_df.cluster_depth.values > callosum_middle_index\n",
    "        striatum_index = mua_df.cluster_depth.values < callosum_middle_index\n",
    "\n",
    "        positions_ = ['striatum'] * len(mua_df.cluster_id)\n",
    "        for index, item in enumerate(positions_):\n",
    "            if cortex_index[index]:\n",
    "                positions_[index] = 'm_cortex'\n",
    "\n",
    "        mua_df['Region'] = positions_\n",
    "\n",
    "\n",
    "\n",
    "        print('Total mua clusters = ' + str(len(spike_vectors)))\n",
    "\n",
    "        ### save out data \n",
    "\n",
    "        if type_ == 'kilosort3':\n",
    "            if not os.path.isdir(out_path +'//non_curated_spikes//'):\n",
    "                os.mkdir(out_path +'//non_curated_spikes//')\n",
    "            good_df.to_csv(out_path +'//non_curated_spikes/good_units_df.csv')\n",
    "            mua_df.to_csv(out_path +'//non_curated_spikes/multiunits_df.csv')\n",
    "\n",
    "        elif type_ == 'kilosort_curated':\n",
    "            if not os.path.isdir(out_path +'//curated_spikes//'):\n",
    "                os.mkdir(out_path +'//curated_spikes//')\n",
    "            good_df.to_csv(out_path +'//curated_spikes/good_units_df.csv')\n",
    "            mua_df.to_csv(out_path +'//curated_spikes/multiunits_df.csv')\n",
    "\n",
    "        else:\n",
    "            print('file wrong name')\n",
    "            \n",
    "        print('data saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb1d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### How many units in each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c115cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "\n",
    "\n",
    "animal = 'EJT136_implant1'\n",
    "input_ = '\\\\non_curated_spikes\\\\' \n",
    "input_ = '\\\\curated_spikes\\\\' \n",
    "\n",
    "path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\' + animal + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3adbc801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJT136_implant1\n",
      "recording2_10-11-2021\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\EJT136_implant1\\\\recording2_10-11-2021\\\\ephys\\\\curated_spikes\\\\good_units_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(recording)\n\u001b[0;32m      5\u001b[0m current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,recording,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mephys\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m input_  \n\u001b[1;32m----> 7\u001b[0m good_units \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgood_units_df.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      8\u001b[0m mua_units \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(current_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiunits_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood units = \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(good_units)))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\EJT136_implant1\\\\recording2_10-11-2021\\\\ephys\\\\curated_spikes\\\\good_units_df.csv'"
     ]
    }
   ],
   "source": [
    "print(animal)\n",
    "for recording in os.listdir(path):\n",
    "    if not 'Store' in recording: # ignore ds store thing\n",
    "        print(recording)\n",
    "        current_path = os.path.join(path,recording,'ephys') + input_  \n",
    "        \n",
    "        good_units = pd.read_csv(current_path + 'good_units_df.csv') \n",
    "        mua_units = pd.read_csv(current_path + 'multiunits_df.csv') \n",
    "        \n",
    "        print('good units = ' + str(len(good_units)))\n",
    "        print('good units = ' + str(len(mua_units))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57d355cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da62023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>981</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>982</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>983</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>984</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>985</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cluster_id group\n",
       "0             0  good\n",
       "1             1  good\n",
       "2             3  good\n",
       "3             4  good\n",
       "4             5   mua\n",
       "..          ...   ...\n",
       "843         981   mua\n",
       "844         982  good\n",
       "845         983  good\n",
       "846         984  good\n",
       "847         985  good\n",
       "\n",
       "[848 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6216380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_label_df1 = pd.read_csv(current_path + 'cluster_KSLabel.tsv', sep='\\t')\n",
    "ks_label_df2 = pd.read_csv(current_path + 'cluster_group.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fdb70d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ks_label_df1.KSLabel == 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af477b0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-b3cd8f19bbc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mks_label_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "sum(ks_label_df2.group == 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9d85c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>KSLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>977</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>978</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>979</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>980</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>mua</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cluster_id KSLabel\n",
       "0             0    good\n",
       "1             1    good\n",
       "2             2     mua\n",
       "3             3    good\n",
       "4             4    good\n",
       "..          ...     ...\n",
       "977         977     mua\n",
       "978         978     mua\n",
       "979         979    good\n",
       "980         980     mua\n",
       "981         981     mua\n",
       "\n",
       "[982 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_label_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7200794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
