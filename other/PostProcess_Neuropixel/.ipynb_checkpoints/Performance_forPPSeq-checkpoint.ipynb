{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Make notebook wider:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Fs = 30000.0\n",
    "\n",
    "def parts(list_, indices):\n",
    "    indices = [0]+indices+[len(list_)]\n",
    "    return [list_[v:indices[k+1]] for k, v in enumerate(indices[:-1])]\n",
    "\n",
    "def RemoveSlowSequences(split,split2):\n",
    "    timefiltered_split = []\n",
    "    for i,item in enumerate(split2):\n",
    "        if item[0] == 1:\n",
    "            timefiltered_split = timefiltered_split + [split[i]]\n",
    "\n",
    "    return(timefiltered_split)\n",
    "\n",
    "def aligntofirstpokeandremovesingletransits(timesplitseqs,timesplitlatencies):\n",
    "    \n",
    "    newseqs = []\n",
    "    newlatencies = []\n",
    "    # align to first poke:\n",
    "    for index_1,fragments in enumerate(timesplitseqs):\n",
    "        current_newseqs = []\n",
    "        current_newlatencies = []\n",
    "        count = -1\n",
    "        seqs = False\n",
    "        for index_2,sequence in enumerate(fragments):\n",
    "            for index_3,transit in enumerate(sequence):\n",
    "                if not str(transit)[0] == str(transit)[1]: # remove repeat pokes\n",
    "                    if str(transit)[0] == '2':\n",
    "                        seqs = True\n",
    "                        current_newseqs = current_newseqs + [[]]\n",
    "                        current_newlatencies = current_newlatencies + [[]]\n",
    "                        count = count + 1\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]\n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "                    elif seqs == True:\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]   \n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "            seqs = False\n",
    " \n",
    "        newseqs = newseqs + [current_newseqs]\n",
    "        newlatencies = newlatencies + [current_newlatencies]\n",
    "    return(newseqs,newlatencies)\n",
    "\n",
    "def generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter):\n",
    "\n",
    "    count = counter\n",
    "    transits= []\n",
    "    trial_number= []\n",
    "    for fragment in processed_seqs:\n",
    "        count = count + 1\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for transit in sequence:\n",
    "                    trial_number = trial_number + [count]\n",
    "                    transits = transits + [transit]\n",
    "        else: ### deals with cases where there are no good transitions in a trial \n",
    "            transits = transits + ['nan']\n",
    "            trial_number = trial_number + [count]\n",
    "\n",
    "    times = []\n",
    "    for fragment in processed_latencies:\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for time in sequence:\n",
    "                    times = times + [time]\n",
    "        else:\n",
    "            times = times + ['nan']\n",
    "\n",
    "    Processesed_Transition_Latencies = pd.DataFrame({'Trial': trial_number, 'Transitions' : transits,'Latencies' : times})\n",
    "\n",
    "    return(Processesed_Transition_Latencies,count)\n",
    "\n",
    "def sequence_contains_sequence(haystack_seq, needle_seq):\n",
    "    for i in range(0, len(haystack_seq) - len(needle_seq) + 1):\n",
    "        if needle_seq == haystack_seq[i:i+len(needle_seq)]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "\n",
    "\n",
    "animal = 'EJT149_implant2'\n",
    "\n",
    "path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\' + animal + '\\\\'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording1_30-11-21\n"
     ]
    }
   ],
   "source": [
    "for recording in os.listdir(path):\n",
    "    if not 'Store' in recording: # ignore ds store thing\n",
    "        print(recording)\n",
    "        current_path = os.path.join(path,recording,'behav_sync') + '\\\\'\n",
    "        for file in os.listdir(current_path):\n",
    "            if 'task' in file:\n",
    "                current_path =  os.path.join(current_path,file) + '\\\\'          \n",
    "        sync_data =  pd.read_csv(current_path + 'Transition_data_sync.csv')\n",
    "        sync_data2 =  pd.read_csv(current_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "        \n",
    "        output_path = os.path.join(path,recording,'post_process_ppseq') + '\\\\' \n",
    "\n",
    "        # these are important for concainating trials later on!\n",
    "        counter1 = -1\n",
    "        counter2 = -1\n",
    "\n",
    "        #split data by trials \n",
    "        trial_split_data = dict(tuple(sync_data.groupby('Trial_id')))\n",
    "\n",
    "        # pull out transitions and timefilter data for each trial:\n",
    "        transitions = []\n",
    "        Tfilters= [[],[]]\n",
    "        latencies = [[],[]]\n",
    "        for i in trial_split_data:\n",
    "            transitions = transitions + [list(trial_split_data[i].loc[:,'Transition_type'])]\n",
    "            Tfilters[0] = Tfilters[0] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_out_in'])]\n",
    "            latencies[0] = latencies[0] +[list(trial_split_data[i].loc[:,'out_in_Latency'])]   \n",
    "            # in in\n",
    "            Tfilters[1] = Tfilters[1] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_in_in'])]\n",
    "            latencies[1] = latencies[1] +[list(trial_split_data[i].loc[:,'in_in_Latency'])]    \n",
    "\n",
    "        # for each trial,remove transntions that were too long and split into reaminign time relevant fragments - but for both latency types, hence the loop\n",
    "        timesplitseqs = [[],[]]\n",
    "        for i in range(2):\n",
    "            Tfilt = Tfilters[i] # use out to in pokes first then in in .\n",
    "            for trial_index,time_filter in enumerate(Tfilt):\n",
    "                start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                split = parts(transitions[trial_index],list(np.array(start_end_inds)+1))\n",
    "                split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                del split[::2] # remove every 2nd item eg. all the transitions that were timefilter = 0 so were too long. \n",
    "                timesplitseqs[i] = timesplitseqs[i] + [TfiltSplit]\n",
    "\n",
    "        ## do the exact same for latency - but for both latency types, hence the loop:\n",
    "        timesplitlatencies = [[],[]]\n",
    "        for i in range(2):\n",
    "            Tfilt = Tfilters[i] \n",
    "            latency = latencies[i]\n",
    "            for trial_index,time_filter in enumerate(Tfilt):\n",
    "                start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                split = parts(latency[trial_index],list(np.array(start_end_inds)+1))\n",
    "                split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                del split[::2] # remove every 2nd item eg. all the latencies that were timefilter = 0 so were too long. \n",
    "                timesplitlatencies[i] = timesplitlatencies[i] + [TfiltSplit]\n",
    "\n",
    "        # for fragments in each trial,sort and trim so that seqs start at initiation port poke and then remove fragments that are too short. ie. remove any transitions sequences that dont inlcude the first port or are just a single transition.\n",
    "        processed_seqs,processed_latencies = aligntofirstpokeandremovesingletransits(timesplitseqs[0],timesplitlatencies[0])  ## use  timesplitlatencies[0] for Out to in Transition times \n",
    "\n",
    "        ## generate processed transition times dataframe:\n",
    "        Processesed_Transition_Latencies_df,counter1 = generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter1)\n",
    "\n",
    "        ## determine perfect sequences and correspondng training level and shaping parameters\n",
    "        trial_perfects = []\n",
    "        T_CorrectScores = [[],[],[],[]]\n",
    "        T_RepeatScores = [[],[],[],[]]\n",
    "\n",
    "        for trial_index,fragments in enumerate(processed_seqs):\n",
    "            perfect = []\n",
    "            for fragment in fragments:\n",
    "                if sequence_contains_sequence(fragment,[21, 16, 63, 37]):\n",
    "                    perfect = perfect + [1]\n",
    "                else:\n",
    "                    perfect = perfect + [0]\n",
    "\n",
    "            trial_perfects = trial_perfects + [perfect]  \n",
    "\n",
    "        # calculate mean for each trial:\n",
    "        perfectscore_trials = []\n",
    "        for trial in trial_perfects:\n",
    "            if len(trial) == 0:\n",
    "                perfectscore_trials = perfectscore_trials + [0]\n",
    "            else:\n",
    "                perfectscore_trials = perfectscore_trials + [np.mean(trial)]\n",
    "\n",
    "        first_p_ephys_time = sync_data2.FirstPoke_EphysTime.values\n",
    "        first_p_ephys_time = first_p_ephys_time[~np.isnan(first_p_ephys_time)]\n",
    "        \n",
    "#         ### just for EJT148 because of missing p1in times:\n",
    "#         first_p_ephys_time =[]\n",
    "#         for i in trial_split_data:\n",
    "#             pokein_times = trial_split_data[i].P1_IN_Ephys_TS.values\n",
    "#             port2in = np.where(trial_split_data[i].Start_Port==2)[0][0]\n",
    "#             first_p_ephys_time = first_p_ephys_time + [pokein_times[port2in]]\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1,figsize=(20,10))\n",
    "        ax.plot(first_p_ephys_time,convolve_movmean(perfectscore_trials,20))\n",
    "        ax.set_xlabel('trials',fontsize = 15)\n",
    "        ax.set_ylabel('performance score',fontsize = 15)\n",
    "        SaveFig('Performance score.png',output_path)\n",
    "\n",
    "        out_df = pd.DataFrame({'ephys_time' : first_p_ephys_time ,\n",
    "                        'Convolved_perfromance_score' : convolve_movmean(perfectscore_trials,20)})\n",
    "\n",
    "        out_df.to_csv(output_path + '/Performance_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\EJT148_implant2\\\\recording2_19-10-20\\\\post_process_ppseq\\\\'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_poke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2706.1355, 2707.28  , 2708.1408, 2708.8907, 2709.935 , 2710.9451,\n",
       "       2713.3526, 2713.6549])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokein_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port2in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.53073333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_split_data[i].P1_IN_Ephys_TS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.where(trial_split_data[i].Start_Port==2)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Trial_id',\n",
       " 'Transition_type',\n",
       " 'Start_Port',\n",
       " 'End_Port',\n",
       " 'Start_Poke_in_time',\n",
       " 'Start_Poke_out_time',\n",
       " 'End_Poke_in_time',\n",
       " 'End_Poke_out_time',\n",
       " 'out_in_Latency',\n",
       " 'in_in_Latency',\n",
       " '2s_Time_Filter_out_in',\n",
       " '2s_Time_Filter_in_in',\n",
       " 'Port_2-3-4-5_LED_intensities',\n",
       " 'Port_1-2-3-4_RewardAmount',\n",
       " 'Repeat_Filter',\n",
       " 'TrialStart_EphysTime',\n",
       " 'FirstPoke_EphysTime',\n",
       " 'P1_IN_Ephys_TS',\n",
       " 'P1_OUT_Ephys_TS',\n",
       " 'P2_IN_Ephys_TS',\n",
       " 'P2_OUT_Ephys_TS',\n",
       " 'backcam_trialstart_seconds',\n",
       " 'backcam_trialstart_timestamps',\n",
       " 'backcam_aligned_P1In_times',\n",
       " 'backcam_aligned_P1Out_times',\n",
       " 'backcam_aligned_P2In_times',\n",
       " 'backcam_aligned_P2Out_times',\n",
       " 'sidecam_trialstart_seconds',\n",
       " 'sidecam_trialstart_timestamps',\n",
       " 'sidecam_aligned_P1In_times',\n",
       " 'sidecam_aligned_P1Out_times',\n",
       " 'sidecam_aligned_P2In_times',\n",
       " 'sidecam_aligned_P2Out_times',\n",
       " 'bottomcam_firstpokeinitiation_times',\n",
       " 'bottomcam_firstpokeinitiation_timestamps',\n",
       " 'bottomcam_aligned_P1In_times',\n",
       " 'bottomcam_aligned_P1Out_times',\n",
       " 'bottomcam_aligned_P2In_times',\n",
       " 'bottomcam_aligned_P2Out_times']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sync_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_data2.FirstPoke_EphysTime.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Trial_id',\n",
       " 'Trial_Start',\n",
       " 'Port',\n",
       " 'PokeIn_Time',\n",
       " 'PokeOut_Time',\n",
       " 'Reward_Times',\n",
       " 'Trial_End',\n",
       " 'Port_2-3-4-5_LED_intensities',\n",
       " 'Port_1-2-3-4_RewardAmount',\n",
       " 'TrainingLevel',\n",
       " 'TrialStart_EphysTime',\n",
       " 'FirstPoke_EphysTime',\n",
       " 'PokeIN_EphysTime',\n",
       " 'PokeOUT_EphysTime',\n",
       " 'backcam_trialstart_seconds',\n",
       " 'backcam_aligned_pokein_times',\n",
       " 'backcam_aligned_pokeout_times',\n",
       " 'backcam_trialstart_timestamps',\n",
       " 'sidecam_trialstart_seconds',\n",
       " 'sidecam_aligned_pokein_times',\n",
       " 'sidecam_aligned_pokeout_times',\n",
       " 'sidecam_trialstart_timestamps',\n",
       " 'bottomcam_firstpokeinitiation_times',\n",
       " 'bottomcam_aligned_pokein_times',\n",
       " 'bottomcam_aligned_pokeout_times',\n",
       " 'bottomcam_firstpokeinitiation_timestamps']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sync_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
